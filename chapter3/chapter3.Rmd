---
title: "ScPoEconometrics"
subtitle: "Session 3 : Simple linear Regression"
author: "F. Oswald, G. Kenedi and P. Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(magrittr)
library(repmis)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(haven)
```

# Recap from from past Lectures and plan for this session

**Recap** 

* R basics : first steps with R studio, importing data, data wrangling 

* Summarising data and more advanced R (`ggplot`, `dplyr`) 

--

**Plan for today**

* Introduction to Simple Linear Regression Model

* Empirical applications

  - **Class size** and **students performance**
  - **Education** and **wage**

---

# Student performance and class size : Data

Let's look at the relation between **student performance and class size** first.

The data we use consists of test scores in fifth grade classes at public elementary schools in Israel. These data were originally used in [Angrist and Lavy (1999)](https://www.dropbox.com/s/le8c38jrzma656t/angrist-lavy.pdf?dl=0)

 **TASK for you**
 
* First, load the data from [here](https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1). (*Hint : you will need a special function to read* ** *.dta* **  *formated data*)

```{r, echo=FALSE}
grades = read_dta(file = "https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1") %>%
  filter(classize > 10 & classize < 40) 
```

* Describe the dataset : What informations do we have? What is an observation here? ...

* What would you do to get an idea of the relationship between grades and class size? 

---

# Student performance and class size : Data 

Let's aggregate the grades by class size and compute standardized scores. 

 **TASK for you**

* Add new variables (`std_math` and `std_verb`) that gives both the standardized math and verbal score.

* Create a new dataset (`agg_grades`) where these standardized scores are averaged by class size. (*Hint : `group_by` class size and `summarise` using the `mean` function*)

* Add a new variable `std_score` that give a unique score value to each class size.

```{r, echo = FALSE}
agg_grades = grades %>%
  mutate(std_math = (avgmath - mean(avgmath))/sd(avgmath), 
         std_verb = (avgverb - mean(avgverb))/sd(avgverb)) %>%
  group_by(classize) %>%
  summarise(std_math = mean(std_math),
            std_verb = mean(std_verb)) %>%
  mutate(std_score = (std_math+std_verb)/2, 
         std_score_bis = 100*(std_score - min(std_score))/(max(std_score)- min(std_score)))
```

* How do the grades seem to be related to this score? 

---

# Student performance and class size : Plot 

Let's vizualise the relationship between class size and student achievments.

.pull-left[

Plots of the raw data : 
```{r,echo=FALSE,fig.align='left',fig.height=5,fig.width=7}
# `r emo::ji("anguished")`
g_math = ggplot(grades, aes(x = classize, y = avgmath)) + 
    geom_point(size = 0.75) +
    labs(x = "class size",y = "Average math score") +
    theme_bw()
g_verb = ggplot(grades, aes(x = classize, y = avgverb)) + 
    geom_point(size = 0.75) +
    labs(x = "class size",y = "Average verbal score") +
    theme_bw()
grid.arrange(g_math,g_verb, nrow = 2)
```
]

--

.pull-right[

Plot after tyding the data : 
```{r,echo=FALSE,fig.align='right',fig.height=5,fig.width=7}
# `r emo::ji("happy")`
ggplot(agg_grades, aes(x = classize, y = std_score)) + 
  geom_point() +
  labs(x = "class size",y = "Average std. score") +
  theme_bw()
```

]

---

# Student performance and class size

How to summarize the relationship : **a line throught the scatterplot** 

.left-wide[
```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=6}
ggplot(agg_grades, aes(x = classize, y = std_score)) + 
  geom_hline(yintercept = -0.25, col = "red") +
  geom_point() +
  labs(x = "class size",y = "Average std. score") +
  theme_bw()
```
]

--

.right-thin[
<br>
<br>

* A *line*! Great. But **which** line? This one?

* That's a *flat* line. But `std_score` is increasing. 

* `r emo::ji("weary")`

]

---

# Student performance and class size

How to summarize the relationship : **a line throught the scatterplot** 

.left-wide[
```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=6}
ggplot(agg_grades, aes(x = classize, y = std_score)) + 
  geom_point() + 
  theme_bw() +
  labs(x = "class size",y = "Average std. score") +
  geom_abline(intercept = -1.5,slope = 0.05, col = "red")
```
]

--

.right-thin[

<br>

* **That** one?

* Slightly better. Has a **slope** and an **intercept**. emo::ji("neutral_face")

* We need a rule to decide! 

]


---

# Writing Down A *Line* (1/2)

Let's formalise a bit what we are doing so far. 

* We are interested in the relationship between two variables for which we observe several *individual* $(x_i,y_i)$. 

--

* We summarise this relationship with a line (for now).

--

* The equation for such a line with intercept $b_0$ and slope $b_1$ is:
    $$
    \widehat{y}_i = b\_0 + b\_1 x\_i
    $$

--

* $\widehat{y}_i$ is our *prediction* for $y_i$ given our model (i.e. the line).

--

* Most of the times, $\widehat{y}_i \neq y_i$, i.e. we make an *error*.

---

# Writing Down A *Line* (2/2)

</br>

* At point $x_i$ we make error $e_i$.

</br>

--

* The *actual data* $(y_i,x_i)$ can thus be written like *prediction + error*:
    $$
    y_i = b_0 + b_1 x_i + e_i
    $$
    
</br>

--

* Our *aim* : Find the value for $b_0$ and $b_1$ that **make the errors as small as possible**. In addition we'll check if it **gives a reasonable description of the data**


---

# Making Errors

```{r, echo = FALSE, message = FALSE, warning = FALSE}
generate_data = function(int = 0.5,
                         slope = 1,
                         sigma = 10,
                         n_obs = 9,
                         x_min = 0,
                         x_max = 10) {
  x = seq(x_min, x_max, length.out = n_obs)
  y = int + slope * x + rnorm(n_obs, 0, sigma)
  fit = lm(y ~ x)
  y_hat = fitted(fit)
  y_bar = rep(mean(y), n_obs)
  error = resid(fit)
  meandev = y - y_bar
  data.frame(x, y, y_hat, y_bar, error, meandev)
}

plot_total_dev = function(reg_data,title=NULL) {
  if (is.null(title)){
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4, col = "black")
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA)
  } else {
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey",main=title,ylim=c(-2,10.5))
     axis(side=2,at=seq(-2,10,by=2))
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA)
  }
  # arrows(reg_data$x, reg_data$y_bar,
  #        reg_data$x, reg_data$y,
  #        col = 'grey', lwd = 1, lty = 3, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 4,col = "black")
  # abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_total_dev_prop = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 1, lty = 2, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_unexp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 3,asp=1)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'red', lwd = 4, lty = 1, length = 0.1, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
}

plot_unexp_SSR = function(reg_data,asp=1,title=NULL) {
  if (is.null(title)){
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA),asp=asp)
      abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
  } else {
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA),asp=asp,main=title)
    axis(side=2,at=seq(-2,10,by=2))
      abline(lm(y ~ x, data = reg_data), lwd = 2, col = "black")
  }
}

plot_exp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSReg (Sum of Squares Regression)",
  xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
  abline(h = mean(reg_data$y), col = "grey")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(21)
plot_data = generate_data(sigma = 2)
```

.left-wide[
```{r line-arrows, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height=5.5,fig.width=8}
par(mar = lowtop)
plot_unexp_dev(plot_data)
par(mar = om)
```
]

.right-thin[
* Red Arrows are *errors* or *residuals* for each prediction.

* Often denoted $u$ or $e$.

* Note that we have both $e>0$ and $e<0$!
]

---
class: inverse

# App Time!

* Let's try to find the best line by minimizing the sum of the errors $\sum^n_{i=1}{e_i}$!

```{r,eval=FALSE}
library(ScPoEconometrics) # load our library
launchApp('reg_simple_arrows')
aboutApp('reg_simple_arrows') # explainer about app
```

---

# Writing Down *The Best* Line

.pull-left[

</br>

* choose $(b_0,b_1)$ s.t. the sum $e_1^2 + \dots + e_N^2$ is **as small as possible**

* $e_1^2 + \dots + e_N^2$ is the *sum of squared residuals*, or SSR.

* Wait a moment... Why *squared* residuals?!
]

--

.pull-right[

</br>

* In previous plot, errors of different sign $(+/-)$ cancel out!

* This makes it hard to find a good line.

* Squaring each $e_i$ solves that issue as $e_i^2 \geq 0, \forall i$.
]


---

# Best Line and Squared Errors

.left-wide[
```{r line-squares, echo=FALSE, message=FALSE, warning=FALSE,fig.width=8,fig.height = 5.5}
par(mar = lowtop)
plot_unexp_SSR(plot_data)
par(mar = om)
```
]

--

.right-thin[
<br>
<br>

* **That's**  the one!

* Perfect! Minimizes the sum of squares.

* emo::ji("relieved")

]

---
class: inverse

# App Time!


Let's minimize some squared errors! 
```{r,eval=FALSE}
launchApp('reg_simple')
aboutApp('reg_simple')
```


---

# Ordinary Least Squares (OLS)

.pull-left[
* OLS estimates is precisely doing it for us, that is minimizing the squared errors.

* But what is the formulae for our *line* parameters $b_0$ and $b_1$?

* In our single regressor case, there is a simple formula for the slope:
  $$
  b_1 = \frac{cov(x,y)}{var(x)}
  $$
  
* and for the intercept
  $$
  b_0 = \bar{y} - b_1\bar{x}
  $$

* emo::ji("rotating_light") You **must** know and understand those formulae!

]

--

.pull-right[

**Interpretation** (for now we assume x and y to be numerical). 

* Intercept $(b_0)$ : The predicted value of $y$ if $x$ is set to 0

* Slope $(b_1)$ : The change in $y$ associated to an increase of $x$ by one unit.

Remember that OLS fits the best **line**!
]
  
---

# OLS : class size and average math score

Let's take back our grades-class size example!

.pull-left[

```{r echo=FALSE, fig.width=5,fig.height = 4, fig.align="left"}
ggplot(agg_grades, aes(x = classize, y = std_score)) + 
  geom_point() + 
  theme_bw() + geom_smooth(method = "lm", se = F, col = "green") + 
  labs(x = "class size",y = "Average std. score") +
  theme_classic()
```
]

--

.pull-right[

* **OLS in `R`**. You can run an OLS regression by executing this command

```{r echo=T, eval = F}
reg = lm(std_score~classize,agg_grades) # assign the result of the regression to the "reg" object
summary(reg) # to view the output 
```


* We get theses values for $b_0$ and $b_1$

```{r echo=FALSE}
coeffs = lm(std_score~classize,agg_grades)$coefficients
coeffs
```

* **Task**: Interpret these two values.

]

---

class: inverse

# App Time!

How does OLS actually perform the minimization problem?

```{r,eval=FALSE}
launchApp('SSR_cone')
aboutApp('SSR_cone')  # after
```


---
class: inverse

# App Time!

Let's do some more OLS!

```{r,eval=FALSE}
launchApp('reg_full')
aboutApp('reg_full')  # after
```

---

# OLS Restrictions #1

**OLS without regressors**

--

.pull-left[
* Our line is flat at level $b_0$:
  $$y_i = b_0$$
* Our optimization problem is now
  $$b_0 = \arg\min_{\text{int}} \sum_{i=1}^N \left[y_i - \text{int}\right]^2,$$
* With solution
  $$b_0 = \frac{1}{N} \sum_{i=1}^N y_i = \overline{y}.$$
]

--

.pull-right[

* In other words: OLS estimates the **mean** of $y$!

```{r,echo = FALSE, fig.height = 4, fig.width=7}
par(mar = lowtop)
plot_total_dev(plot_data)
par(mar = om)
```

]


---

class: inverse

# OLS Restrictions #2

**OLS without intercept**

* Rahter than imposing no regressor we can see what happens with **no intercept** in the model. 

```{r,eval=FALSE}
launchApp('reg_constrained')
aboutApp('reg_constrained')  # after
```


---

# Other OLS variations

* 

* They are described [in the book](https://scpoecon.github.io/ScPoEconometrics/linreg.html#OLS). Optional emo::ji("nerd_face").

* There is an app for each of them:
<br>
<br>

type | App  
-------- | --------
No Intercept | `launchApp('reg_constrained')` 
Centered Regression | `launchApp('demeaned_reg')` 
Standardized Regression | `launchApp('reg_standardized')`


---

# Predictions and Residuals


1. The error is $e_i = y_i - \widehat{y}_i$

2. The average of $\widehat{y}_i$ is equal to $\bar{y}$.
    $$\begin{align}\frac{1}{N} \sum_{i=1}^N \widehat{y}_i &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\ &= b_0 + b_1  \bar{x}  = \bar{y} \end{align}$$
--

3. Then,
    $$\frac{1}{N} \sum_{i=1}^N e_i = \bar{y} - \frac{1}{N} \sum_{i=1}^N \widehat{y}_i = 0$$
    i.e. the average of errors is zero.

---


# Properties of Residuals

.pull-left[
1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).

Let's look at the data behind our *arrows* plot above:

]

--

.pull-right[
```{r,echo=FALSE}
sd = subset(plot_data,select=c(x,y,y_hat,error))
sd = rbind(sd, colMeans(sd))
sd = cbind(c(rep("",nrow(sd)-1),"Means"),sd)
names(sd)[1] = ""
ss = hux(sd) 

ss %>%
  add_colnames() %>%
  set_number_format(2) %>%
  set_bottom_border(c(1,nrow(sd)), 2:5,2) %>%
  set_align(row = 1,everywhere, "center")

```

]

---

# Properties of Residuals

.pull-left[

1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).
]

--

.pull-right[
```{r}
# 1.
all.equal(mean(sd$y_hat), mean(sd$y))

# 2.
all.equal(mean(sd$error), 0)

# 3.
all.equal(cov(sd$error,sd$y_hat), 0)
```
]

---


# Linear Statistics

* It's important to keep in mind that Var, Cov, Corr and Regression measure **linear relationships** between two variables.

* Two datasets with *identical* correlations could look *vastly* different.

* They would have the same regression line.

* Same correlation coefficient.

--

* Is that even possible?

---

# Linear Statistics: Anscombe

* Francis Anscombe (1973) comes up with 4 datasets with identical stats. But look!

.left-wide[

```{r,echo=FALSE,fig.height = 4}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
covs = data.frame(dataset = 1:4, cov = 0.0)
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  covs[i,"cov"] = eval(parse(text = paste0("cov(anscombe$x",i,",anscombe$y",i,")")))
  covs[i,"var(y)"] = eval(parse(text = paste0("var(anscombe$y",i,")")))
  covs[i,"var(x)"] = eval(parse(text = paste0("var(anscombe$x",i,")")))
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 0, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
  abline(mods[[i]], col = "green")
}
par(op)
```
]

--

.right-thin[
```{r,echo = FALSE}
ch = hux(covs)
ch %>%
  set_number_format(row = everywhere, col = c(2:4), 2) %>%
  set_number_format(row = everywhere, col = 1, 0) %>%
  add_colnames()
```

]
---

# Dinosaurs in your Data?

* So, be wary of only looking a linear summary stats.

* Also look at plots.

* Dinosaurs?
    ```{r,eval=FALSE}
    launchApp("datasaurus")
    aboutApp("datasaurus")
    ```

---

# Nonlinear Relationships in Data?

* We can accomodate non-linear relationships in regressions.

* We'd just add a higher order term like this:
    $$
    y_i = b_0 + b_1 x_i + b_2 x_i^2 + e_i
    $$
    
* This is *multiple regression* (next chapter!)

---

# Nonlinear Relationships in Data?

* For example, suppose we had this data and fit the above regression:
    ```{r non-line-cars-ols2,echo=FALSE,echo=FALSE,fig.height = 5}
    l1 = lm(mpg~hp+I(hp^2),data=mtcars)
    newdata=data.frame(hp=seq(from=min(mtcars$hp),to=max(mtcars$hp),length.out=100))
    newdata$y = predict(l1,newdata=newdata)
    plot(mtcars$hp,mtcars$mpg,xlab="x",ylab="y",pch = 20, cex = 2)
    grid()
    lines(newdata$hp,newdata$y,lw=3,col = "red")
    ```

---

# Analysis of Variance

* Remember that $y_i = \widehat{y}_i + e_i$.

* We have the following decomposition:
    $$\begin{align} Var(y) &= Var(\widehat{y} + e)\\&= Var(\widehat{y}) + Var(e) + 2 Cov(\widehat{y},e)\\&= Var(\widehat{y}) + Var(e)\end{align}$$
    
* Because: $Cov(\hat{y},e)=0$

* Total variation (SST) = Model explained (SSE) + Unexplained (SSR)


---

# Assessing the Goodness of Fit

* The $R^2$ measures how good the model fits the data.

* $R^2$ close to $1$ indicates a very high explanatory power of the model, $R^2$ close to $0$ means that the variations in the outcome $(y)$ are very poorly captured.
    $$
    R^2 = \frac{\text{variance explained}}{\text{total variance}} =     \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
    $$
    
* NB: Small $R^2$ doesn't mean it's a useless model!

---

# An Example - Log Wage Equation

Let's consider the following example concerning wage data collected in the [1976 Current Population Survey](https://www.census.gov/programs-surveys/cps/library.1976.html) in the USA. 

We want to investigate the relationship between average hourly earnings, and years of education. 

* The data come from the `wooldridge` package. Load it into your global environment (`.GlobalEnv`) with the function `data()`.

```{r echo = FALSE}
data("wage1", package = "wooldridge")   # load data

# a function that returns a plot
plotfun <- function(wage1,log=FALSE,rug = TRUE){
    y = wage1$wage
    if (log){
        y = log(wage1$wage)
    }
    plot(y = y,
       x = wage1$educ, 
       col = "red", pch = 21, bg = "grey",     
       cex=1.25, xaxt="n", frame = FALSE,      # set default x-axis to none
       main = ifelse(log,"log(Wages) vs. Education, 1976","Wages vs. Education, 1976"),
       xlab = "years of education", 
       ylab = ifelse(log,"Log Hourly wages","Hourly wages"))
    axis(side = 1, at = c(0,6,12,18))         # add custom ticks to x axis
    if (rug) rug(wage1$wage, side=2, col="red")        # add `rug` to y axis
}
```

* Briefly describe the data and produce a plot to vizualize the link between `wage` and `education`.
--
```{r echo = FALSE, fig.height = 3, fig.width=10}
par(mfcol = c(1,2))  # set up a plot with 2 panels
# plot 1: standard scatter plot
plotfun(wage1)

# plot 2: add a panel with histogram+density
hist(wage1$wage,prob = TRUE, col = "grey", border = "red", 
     main = "Histogram of wages and Density",xlab = "hourly wage")
lines(density(wage1$wage), col = "black", lw = 2)
```
---

# An Example - Log Wage Equation

OLS regression : let's regress the wage on the education. For this you have to use the `lm()` function.

--

```{r}
my_reg = lm(wage ~ educ, data = wage1)
```

--

We can add the resulting regression line to our last plot

.pull-left[

```{r, eval = FALSE, out.width=3}
plotfun(wage1)
abline(my_reg, col = 'black', lw = 2) # add regression line
```
]

--

.pull-right[
```{r echo = FALSE, fig.height=3.5, fig.width=6}
plotfun(wage1)
abline(my_reg, col = 'black', lw = 2) # add regression line
```

]


---
class: inverse

# An Example - Log Wage Equation : Task 

**Questions : Interpretation**

Here is the summary of the OLS regression. 

.pull-left[

```{r}
summary(my_reg)
```
]

--

.pull-right[

1) With zero year of education, the hourly wage is about **??** dollars per hour.

2) Each additional year of education increase hourly wage by **??** cents.

3) Compute the predicted wage associated to 15 years of education.
]

---
class: inverse 

# An Example - Log Wage Equation : Task

**Answers** 

Answer 1) -0.9 dollars per hour (row named `(Intercept)`)

--

Answer 2) Each additional year of education increase hourly wage by 54 cents. (row named `educ`)

--

Answer 3) We predict roughly **-0.9** + 0.541 * 15 = 7.215 dollars/h.

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |
