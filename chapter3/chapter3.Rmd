---
title: "ScPoEconometrics"
subtitle: "Session 3 : Simple linear Regression"
author: "F. Oswald, G. Kenedi and P. Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library("magrittr")
library("repmis")
library("dplyr")
library("ggplot2")
library("gridExtra")
library("haven")
library("ggpubr")
library("huxtable")
```

# Recap from from past Lectures and plan for this session

**Recap** 

* R basics, data importing 

* Exploratory data analysis (`ggplot2`, `dplyr`)

    * Data wrangling (subsetting, filtering,...)
    * Summary statistics
    * Data visualization

--

**Plan for today**

* Introduction to Simple Linear Regression Model

* Empirical applications

  - **Class size** and **students performance**
  - **Education** and **wage**

---

# Student performance and class size

Let's look at the relationship between **student performance and class size** first.

This topic has been widely discussed in both the academic and political sphere. Throughout the course we'll see several approaches that have been followed by economists.

For now, the data we will use consist of test scores in fifth grade classes at public elementary schools in Israel. 

These data were originally used in [Angrist and Lavy (1999)](https://www.dropbox.com/s/le8c38jrzma656t/angrist-lavy.pdf?dl=0)

---

class:: inverse

# Student performance and class size : Data

 **TASK 1**
 
* First, load the data from [here](https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1). (*Hint : you will need a special function to read* ** *.dta* **  *formated data*)

```{r, echo=FALSE}
grades = read_dta(file = "https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1") #%>%
#  filter(classize > 10 & classize < 40) 
```

* Describe the dataset : What informations do we have? What defines an observation here? NA's?...

* Do you have any prior about about the actual (linear) relationship between class size and student achievement? What would you do to get a first insight? 

---

# Student performance and class size : first plot  

.left-wide[
```{r, echo=FALSE,fig.height=4,fig.width=6}
g_math = ggplot(grades, aes(x = classize, y = avgmath)) + 
    geom_point(size = 0.5) +
    labs(x = "class size",y = "Average math score") +
    theme_bw()
g_verb = ggplot(grades, aes(x = classize, y = avgverb)) + 
    geom_point(size = 0.5) +
    labs(x = "class size",y = "Average verbal score") +
    theme_bw()
grid.arrange(g_math,g_verb, nrow = 2)
```
]

--

.right-thin[

* Seems to be a positive association but not easy to tell.

* Let's compute the average score by class size! 

]
---

class:: inverse

# Student performance and class size : tidy plot  

Let's aggregate the test scores by class size. *(N.B.: each observation is relative to a specific class, so the raw scores are already averages. Here we average these averages over class size.)*

 **TASK**

* Create a new dataset (`grades_avg_cs`) where math and verbal scores are averaged by class size. Let's call these new average scores `avgmath_cs` and `avgverb_cs`. 

```{r, echo = FALSE}
grades_avg_cs = grades %>%
  group_by(classize) %>%
  summarise(avgmath_cs = mean(avgmath),
            avgverb_cs = mean(avgverb))
```


* Redo the same plots as before. 

---

# Student performance and class size : Plot 

Let's vizualise the relationship between class size and student achievements.

.pull-left[

Plots of the raw data (with `jitter`) 
```{r,echo=FALSE,fig.align='left',fig.height=5,fig.width=7}
# `r emo::ji("anguished")`
g_math = ggplot(grades, aes(x = classize, y = avgmath)) + 
  geom_point(alpha = 0.5, size = 0.5) +
  labs(x = "class size",y = "Average math score") +
  theme_bw() +
  geom_jitter(alpha = 0.5, size = 0.5)
g_verb = ggplot(grades, aes(x = classize, y = avgverb)) + 
  geom_point(alpha = 0.5, size = 0.5) +
  labs(x = "class size",y = "Average verbal score") +
  theme_bw()+
  geom_jitter(alpha = 0.5, size = 0.5)
grid.arrange(g_math,g_verb, nrow = 2)
```
]

--

.pull-right[

Plot after grouping by class size
```{r,echo=FALSE,fig.align='right',fig.height=5,fig.width=7}
# `r emo::ji("happy")`
g_math_cs = ggplot(grades_avg_cs, aes(x = classize, y = avgmath_cs)) + 
    geom_point(size = 0.75) +
    labs(x = "class size",y = "Average math score") +
    theme_bw()
g_verb_cs = ggplot(grades_avg_cs, aes(x = classize, y = avgverb_cs)) + 
    geom_point(size = 0.75) +
    labs(x = "class size",y = "Average verbal score") +
    theme_bw()
grid.arrange(g_math_cs,g_verb_cs, nrow = 2)
```

]

---

# Student performance and class size

How to summarize the relationship : **a line throught the scatterplot** 

.left-wide[
```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=7}
ggplot(grades_avg_cs, aes(x = classize, y = avgmath_cs)) + 
  geom_hline(yintercept = 65, col = "red") +
  geom_point() +
  labs(x = "class size",y = "Average math score") +
  theme_bw()
```
]

--

.right-thin[
<br>
<br>

* A *line*! Great. But **which** line? This one?

* That's a *flat* line. But `avgmath_cs` is increasing with class size. 

* r emo::ji("weary")

]

---

# Student performance and class size

How to summarize the relationship : **a line throught the scatterplot** 

.left-wide[
```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=7}
ggplot(grades_avg_cs, aes(x = classize, y = avgmath_cs)) + 
  geom_point() + 
  theme_bw() +
  labs(x = "class size",y = "Average math score") +
  geom_abline(intercept = 50,slope = 0.6, col = "red")
```
]

--

.right-thin[

<br>

* **That** one?

* Slightly better. Has a **slope** and an **intercept**. emo::ji("neutral_face")

* We need a rule to decide! 

]


---

# Writing Down A *Line* #1

Let's formalise a bit what we are doing so far. 

* We are interested in the relationship between two variables for which we observe several *individual* $(x_i,y_i)$. 

--

* We summarise this relationship with a line (for now).

--

* The equation for such a line with intercept $b_0$ and slope $b_1$ is:
    $$
    \widehat{y}_i = b\_0 + b\_1 x\_i
    $$

--

* $\widehat{y}_i$ is our *prediction* for $y_i$ given our model (i.e. the line).

--

* Most of the times, $\widehat{y}_i \neq y_i$, i.e. we make an *error*.

---

# Writing Down A *Line* #2

</br>

* At point $x_i$ we make error $e_i$.

</br>

--

* The *actual data* $(y_i,x_i)$ can thus be written like *prediction + error*:
    $$
    y_i = b_0 + b_1 x_i + e_i
    $$
    
</br>

--

* Our **goal** : Find the values for $b_0$ and $b_1$ that **make the errors as small as possible**.

* In addition we'll check if it **gives a reasonable description of the data**.


---

# Making Errors

```{r, echo = FALSE, message = FALSE, warning = FALSE}
generate_data = function(int = 0.5,
                         slope = 1,
                         sigma = 10,
                         n_obs = 9,
                         x_min = 0,
                         x_max = 10) {
  x = seq(x_min, x_max, length.out = n_obs)
  y = int + slope * x + rnorm(n_obs, 0, sigma)
  fit = lm(y ~ x)
  y_hat = fitted(fit)
  y_bar = rep(mean(y), n_obs)
  error = resid(fit)
  meandev = y - y_bar
  data.frame(x, y, y_hat, y_bar, error, meandev)
}

plot_total_dev = function(reg_data,title=NULL) {
  if (is.null(title)){
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4, col = "black")
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA)
  } else {
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey",main=title,ylim=c(-2,10.5))
     axis(side=2,at=seq(-2,10,by=2))
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA)
  }
  # arrows(reg_data$x, reg_data$y_bar,
  #        reg_data$x, reg_data$y,
  #        col = 'grey', lwd = 1, lty = 3, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 4,col = "black")
  # abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_total_dev_prop = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 1, lty = 2, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_unexp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 3,asp=1)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'red', lwd = 4, lty = 1, length = 0.1, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
}

plot_unexp_SSR = function(reg_data,asp=1,title=NULL) {
  if (is.null(title)){
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA),asp=asp)
      abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
  } else {
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA),asp=asp,main=title)
    axis(side=2,at=seq(-2,10,by=2))
      abline(lm(y ~ x, data = reg_data), lwd = 2, col = "black")
  }
}

plot_exp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSReg (Sum of Squares Regression)",
  xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
  abline(h = mean(reg_data$y), col = "grey")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(21)
plot_data = generate_data(sigma = 2)
```

.left-wide[
```{r line-arrows, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height=5.5,fig.width=8}
par(mar = lowtop)
plot_unexp_dev(plot_data)
par(mar = om)
```
]

.right-thin[

</br>
</br>

* Red Arrows are *errors* or *residuals* for each prediction.

* *Errors* can be either positive (bottom to top arrows) or negative.

]

---
class: inverse

# App Time!

At first sight, one might try to simply minimize the sum of all the errors
$\sum^n_{i=1}{e_i}$

Let's try to find the best line by minimizing the sum of the errors.

```{r,eval=FALSE}
library(ScPoEconometrics) # load our library
launchApp('reg_simple_arrows')
aboutApp('reg_simple_arrows') # explainer about app
```

---

# Writing Down A *Line* #3


.pull-left[

</br>

* Errors of different sign $(+/-)$ cancel out, so let's consider **squared residuals** 
$$\forall i, e_i^2 = (y_i - b_0 - b1x_i)^2$$

* Choose $(b_0,b_1)$ s.t. the sum $e_1^2 + \dots + e_N^2$ is **as small as possible**

* $e_1^2 + \dots + e_N^2$ is the *sum of squared residuals*, or SSR.
]

--

.pull-right[

</br>

```{r, echo=FALSE, message=FALSE, warning=FALSE,fig.width=6,fig.height = 4.5}
par(mar = lowtop)
plot_unexp_SSR(plot_data)
par(mar = om)
```
]

---

class: inverse

# App Time!

Let's minimize some squared errors! 
```{r,eval=FALSE}
launchApp('reg_simple')
aboutApp('reg_simple')
```

---

# **O**rdinary **L**east **S**quares (OLS) #1

* Minimizing the sum of squared residuals is precisely what **OLS** regression does !! 

* But what is the formulae for $b_0$ (intercpet) and $b_1$ (slope)?

* In our single regressor case, there is a simple formula for the slope:
  $$
  b_1^{OLS} = \frac{cov(x,y)}{var(x)}
  $$
  
* and for the intercept
  $$
  b_0^{OLS} = \bar{y} - b_1\bar{x}
  $$

* N.B. OLS relies on squared residuals, one could have think of using absolute values of errors $|e_i|$ instead, it exist and it is actually another estimator.  

---

class: inverse

# App Time!

How does OLS actually perform the minimization problem?

```{r,eval=FALSE}
launchApp('SSR_cone')
aboutApp('SSR_cone')  # after
```

---
# **O**rdinary **L**east **S**quares (OLS) #2

</br>

**Interpreting** OLS (for now we assume x and y to be numerical). 

* Intercept $(b_0)$ : The predicted value of $y$ if $x$ is set to 0

* Slope $(b_1)$ : The change in $y$ associated to an increase of $x$ by one unit.

Remember that OLS in this case OLS fits the best... *line*, so the interpretation is exactly the same as the one of the intercept and slope of a line.

---

# OLS : class size and student achievement

Let's take back our grades-class size example! We focus on math score here. 

.pull-left[

```{r echo=FALSE, fig.width=6,fig.height = 4}
ggplot(grades_avg_cs, aes(x = classize, y = avgmath_cs)) + 
    geom_point(size = 0.75) +
    geom_smooth(method = "lm", se = F, col = "green") + 
    labs(x = "class size",y = "Average math score") +
    stat_regline_equation(label.x = 30, label.y = 55, col = "darkgreen") +
    theme_bw()

ols_math_classize = lm(avgmath_cs~classize, grades_avg_cs)
```
]

--

.pull-right[

</br>

* You can notice that some points/observations are poorly fitted by the OLS line. 

* We call them **outliers**. 

* Looking at the regression line equation, can you make sens of the two values `r round(ols_math_classize$coefficients[1])` and `r round(ols_math_classize$coefficients[2],2)`?

]

---

# OLS in R : nothing more simple ! 

You can run the OLS regression associated to the equation : 
$$\textrm{avgmath_cs} = b_0 + b_1 \textrm{classsize} + e_i$$

by using the `lm` function : 

```{r echo=T, eval = F}
ols_math_classize = lm(formula = avgmath_cs~classize, data = grades_avg_cs) # assign the result of the regression to the "ols_math_classize" object
```

Then, to obtain a summary of this regression, call the `summary` function : 

```{r echo=TRUE, eval = F}
summary(ols_math_classize)
```

* There is a lot of information given in the summary output, we focus only on the OLS coefficients $(b_o, b_1)$

* The next lectures will allow you to make sens of the whole thing.  

---

# OLS Restriction

**OLS without regressors**

--

.pull-left[
* Our line is flat at level $b_0$:
  $$y_i = b_0$$
* Our optimization problem is now
  $$b_0 = \arg\min_{\text{int}} \sum_{i=1}^N \left[y_i - \text{int}\right]^2,$$
* With solution
  $$b_0 = \frac{1}{N} \sum_{i=1}^N y_i = \overline{y}.$$
]

--

.pull-right[

* In other words: OLS estimates the **mean** of $y$!

```{r,echo = FALSE, fig.height = 4, fig.width=7}
par(mar = lowtop)
plot_total_dev(plot_data)
par(mar = om)
```

]


---

# OLS Variation

TBD

---

# OLS variations

TBD

* They are described [in the book](https://scpoecon.github.io/ScPoEconometrics/linreg.html#OLS). Optional emo::ji("nerd_face").

* There is an app for each of them:
<br>
<br>

type | App  
-------- | --------
No Intercept | `launchApp('reg_constrained')` 
Centered Regression | `launchApp('demeaned_reg')` 
Standardized Regression | `launchApp('reg_standardized')`


---

# Predictions and Residuals


1. The error is $e_i = y_i - \widehat{y}_i$

2. The average of $\widehat{y}_i$ is equal to $\bar{y}$.
    $$\begin{align}\frac{1}{N} \sum_{i=1}^N \widehat{y}_i &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\ &= b_0 + b_1  \bar{x}  = \bar{y} \end{align}$$
--

3. Then,
    $$\frac{1}{N} \sum_{i=1}^N e_i = \bar{y} - \frac{1}{N} \sum_{i=1}^N \widehat{y}_i = 0$$
    i.e. the average of errors is zero.

---


# Properties of Residuals

.pull-left[
1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).

Let's look at the data behind our *arrows* plot above:

]

--

.pull-right[

```{r,echo=FALSE, results="asis"}
sd = subset(plot_data,select=c(x,y,y_hat,error))
sd = rbind(sd, colMeans(sd))
sd = cbind(c(rep("",nrow(sd)-1),"Means"),sd)
names(sd)[1] = ""

ss = hux(sd,add_colnames = T) %>%
  set_number_format(2) %>%
  set_bottom_border(c(1,nrow(sd)), 1:5,2) %>%
  set_align(row = 1,everywhere, "center")
ss
```

]

---

# Properties of Residuals

.pull-left[

1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).
]

--

.pull-right[
```{r}
# 1.
all.equal(mean(sd$y_hat), mean(sd$y))

# 2.
all.equal(mean(sd$error), 0)

# 3.
all.equal(cov(sd$error,sd$y_hat), 0)
```
]

---


# Linear Statistics

* It's important to keep in mind that Var, Cov, Corr and Regression measure **linear relationships** between two variables.

* Two datasets with *identical* correlations could look *vastly* different.

* They would have the same regression line.

* Same correlation coefficient.

--

* Is that even possible?

---

# Linear Statistics: Anscombe

* Francis Anscombe (1973) comes up with 4 datasets with identical stats. But look!

.left-wide[

```{r,echo=FALSE,fig.height = 4}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
covs = data.frame(dataset = 1:4, cov = 0.0)
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  covs[i,"cov"] = eval(parse(text = paste0("cov(anscombe$x",i,",anscombe$y",i,")")))
  covs[i,"var(y)"] = eval(parse(text = paste0("var(anscombe$y",i,")")))
  covs[i,"var(x)"] = eval(parse(text = paste0("var(anscombe$x",i,")")))
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 0, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
  abline(mods[[i]], col = "green")
}
par(op)
```
]

--

.right-thin[
```{r,echo = FALSE}
ch = hux(covs)
ch %>%
  set_number_format(row = everywhere, col = c(2:4), 2) %>%
  set_number_format(row = everywhere, col = 1, 0) %>%
  add_colnames()
```

]
---

# Dinosaurs in your Data?

* So, be wary of only looking a linear summary stats.

* Also look at plots.

* Dinosaurs?
    ```{r,eval=FALSE}
    launchApp("datasaurus")
    aboutApp("datasaurus")
    ```

---

# Nonlinear Relationships in Data?

* We can accomodate non-linear relationships in regressions.

* We'd just add a higher order term like this:
    $$
    y_i = b_0 + b_1 x_i + b_2 x_i^2 + e_i
    $$
    
* This is *multiple regression* (next chapter!)

---

# Nonlinear Relationships in Data?

* For example, suppose we had this data and fit the above regression:
    ```{r non-line-cars-ols2,echo=FALSE,echo=FALSE,fig.height = 5}
    l1 = lm(mpg~hp+I(hp^2),data=mtcars)
    newdata=data.frame(hp=seq(from=min(mtcars$hp),to=max(mtcars$hp),length.out=100))
    newdata$y = predict(l1,newdata=newdata)
    plot(mtcars$hp,mtcars$mpg,xlab="x",ylab="y",pch = 20, cex = 2)
    grid()
    lines(newdata$hp,newdata$y,lw=3,col = "red")
    ```

---

# Analysis of Variance

* Remember that $y_i = \widehat{y}_i + e_i$.

* We have the following decomposition:
    $$\begin{align} Var(y) &= Var(\widehat{y} + e)\\&= Var(\widehat{y}) + Var(e) + 2 Cov(\widehat{y},e)\\&= Var(\widehat{y}) + Var(e)\end{align}$$
    
* Because: $Cov(\hat{y},e)=0$

* Total variation (SST) = Model explained (SSE) + Unexplained (SSR)


---

# Assessing the Goodness of Fit

* The $R^2$ measures how good the model fits the data.

* $R^2$ close to $1$ indicates a very high explanatory power of the model, $R^2$ close to $0$ means that the variations in the outcome $(y)$ are very poorly captured.
    $$
    R^2 = \frac{\text{variance explained}}{\text{total variance}} =     \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
    $$
    
* NB: Small $R^2$ doesn't mean it's a useless model!

---

# An Example - Log Wage Equation

Let's consider the following example concerning wage data collected in the [1976 Current Population Survey](https://www.census.gov/programs-surveys/cps/library.1976.html) in the USA. 

We want to investigate the relationship between average hourly earnings, and years of education. 

* The data come from the `wooldridge` package. Load it into your global environment (`.GlobalEnv`) with the function `data()`.

```{r echo = FALSE}
data("wage1", package = "wooldridge")   # load data

# a function that returns a plot
plotfun <- function(wage1,log=FALSE,rug = TRUE){
    y = wage1$wage
    if (log){
        y = log(wage1$wage)
    }
    plot(y = y,
       x = wage1$educ, 
       col = "red", pch = 21, bg = "grey",     
       cex=1.25, xaxt="n", frame = FALSE,      # set default x-axis to none
       main = ifelse(log,"log(Wages) vs. Education, 1976","Wages vs. Education, 1976"),
       xlab = "years of education", 
       ylab = ifelse(log,"Log Hourly wages","Hourly wages"))
    axis(side = 1, at = c(0,6,12,18))         # add custom ticks to x axis
    if (rug) rug(wage1$wage, side=2, col="red")        # add `rug` to y axis
}
```

* Briefly describe the data and produce a plot to vizualize the link between `wage` and `education`.
--
```{r echo = FALSE, fig.height = 3, fig.width=10}
par(mfcol = c(1,2))  # set up a plot with 2 panels
# plot 1: standard scatter plot
plotfun(wage1)

# plot 2: add a panel with histogram+density
hist(wage1$wage,prob = TRUE, col = "grey", border = "red", 
     main = "Histogram of wages and Density",xlab = "hourly wage")
lines(density(wage1$wage), col = "black", lw = 2)
```
---

# An Example - Log Wage Equation

OLS regression : let's regress the wage on the education. For this you have to use the `lm()` function.

--

```{r}
my_reg = lm(wage ~ educ, data = wage1)
```

--

We can add the resulting regression line to our last plot

.pull-left[

```{r, eval = FALSE, out.width=3}
plotfun(wage1)
abline(my_reg, col = 'black', lw = 2) # add regression line
```
]

--

.pull-right[
```{r echo = FALSE, fig.height=3.5, fig.width=6}
plotfun(wage1)
abline(my_reg, col = 'black', lw = 2) # add regression line
```

]


---
class: inverse

# An Example - Log Wage Equation : Task 

**Questions : Interpretation**

Here is the summary of the OLS regression. 

.pull-left[

```{r}
summary(my_reg)
```
]

--

.pull-right[

1) With zero year of education, the hourly wage is about **??** dollars per hour.

2) Each additional year of education increase hourly wage by **??** cents.

3) Compute the predicted wage associated to 15 years of education.
]

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END


|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |
