<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>ScPoEconometrics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Florian Oswald" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
    <link rel="stylesheet" href="../css/scpo.css" type="text/css" />
    <link rel="stylesheet" href="../css/scpo-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ScPoEconometrics
## Regression Inference
### Florian Oswald
### SciencesPo Paris </br> 2019-11-11

---


layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---



layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---

# Packages used in this set of slides


```r
library(tidyverse)
library(infer)
library(moderndive)
library(wooldridge)
```

---

# Back To Wages!

.pull-left[
&lt;img src="reg_inference_files/figure-html/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

]

--

.pull-right[
* We started this *looong* detour into sampling while talking about **wages**.

* We were interested in the determinants of hourly wage of US workers.

* We used a sample of 526 US workers.

* We had estimated intercept `\(b_0\)` and slope `\(b_1\)`.

* You should be asking now: Is that slope **really** different from zero? Let's look at the model output.


]

---

# Regression Table

.pull-left[

```r
summary(hourly_wage)
```

```
## 
## Call:
## lm(formula = lwage ~ educ, data = wage1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.21158 -0.36393 -0.07263  0.29712  1.52339 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.583773   0.097336   5.998 3.74e-09 ***
## educ        0.082744   0.007567  10.935  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.4801 on 524 degrees of freedom
## Multiple R-squared:  0.1858,	Adjusted R-squared:  0.1843 
## F-statistic: 119.6 on 1 and 524 DF,  p-value: &lt; 2.2e-16
```
]

.pull-right[
* We know how to get *predicted* values from this:
    `$$\hat{y} = b_0 + b_1 x$$`
    
* Filling in our estimates:
    `$$\hat{\text{lwage}} = 0.58 + 0.08 \text{educ}$$`
    
* Remind yourself what both `\(b_0, b_1\)` stand for.
]

---

# Sampling Scenario

.pull-left[
* We took a sample!

* `\(b_0, b_1\)` are **point estimates** much like the sample proportion `\(\hat{p}\)` was!

* In fact, we imagine that our prediction
    `$$\hat{y} = b_0 + b_1 x$$`
    is an **estimate** about an unknown, true population line
    `$$y = \beta_0 + \beta_1 x$$`
    
* We *really* want to know `\(\beta\)`. `\(b\)` is our gest guess.
]

--

.pull-right[
* Sampling implies sampling variation.

* If we collected a *different* sample of 526 workers, would we have obtained **different** estimates `\(b_0, b_1\)`? ðŸ¤”

* Let's bring what we know about Confidence Intervals, Hypothesis tests and Standard Errors to bear on those estimates!
]

---

# Understanding Regression Tables



```r
# get alternative regression table:
moderndive::get_regression_table(hourly_wage)
```
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:center;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; std_error &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; p_value &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; lower_ci &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; upper_ci &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; intercept &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.584 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.097 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 5.998 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.393 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.775 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; educ &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.083 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.008 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 10.935 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.068 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.098 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
--

* We know the meaning of all column names in that table by now (we focus on `\(b_1\)`, case for `\(b_0\)` is identical):


Entry | Meaning
----- | ----
`std_error` |  std.deviation of sampling dist of `\(b_1\)`.
`statistic` |  Test statistic associated to `\(H_0: \beta_1 = 0,H_A:\beta_1 \neq 0\)`
`p value` |  p value associated to `\(H_0:\beta_1 = 0,H_A:\beta_1 \neq 0\)`
`lower_ci, upper_ci` |  range of plausible values for `\(\beta_1\)`

---

# Standard Error of `\(b_1, SE(b_1) = 0.008\)`

.pull-left[
* If we took 1000 samples of workers, we'd get 1000 different estimates for `\(b\)`.

* Those estimates would follow the *sampling distribution of `\(b\)`. 

* Notice that we'd get 1000 different slopes `\(b_1\)` **and** 1000 different intercept `\(b_0\)`!
]

.pull-right[
* The standard error measures the *spread* of the associated sampling distribution.

* Again, theory can tell us how to obtain this measure.

* Again, we will show a quick simulation study, as an alternative.
]


---

# Hypothesis: True `\(\beta = 0\)`?

.pull-left[
* We set up this hypothesis test:
    `$$\begin{align}H_0:&amp; \beta_1 = 0\\H_A:&amp; \beta \neq 0\end{align}$$`
    
* If `\(H_0\)` is true, the true slope is zero: There is in fact **no** relationship between `educ` and `lwage`. In that case observing `\(b_1 &gt; 0\)` was just chance.

* If `\(H_0\)` is false, then there is a true relationship.

* We assume `\(H_0\)` is true, derive the **null distribution** and confront with a test statistic.

]


.pull-right[

* The `statistic` column is *approximately* `\(\frac{b}{SE(b)} = \frac{0.083}{0.008} = 10.375\)`.

## p-value

* The `pvalue` column tells us *how extreme* the estimated slope `\(b_1\)` is in a universe where the truth is `\(\beta_1 = 0\)`

* We see a value of `0` here. So, for any significance level `\(\alpha\)`, we'd reject `\(H_0\)`.

* The pvalue measures the area outside of `\(\pm\)` `statistic` under the Null distribution.
]

---

# Confidence Interval around `\(b_1\)`

.pull-left[
* The values `[lower_ci,upper_ci] =` `\([0.068,0.098]\)` delimite the 95% confidence interval around `\(b_1\)`.

* We said: If we took a large number of samples (1000), in 95% of cases would this interval contain the true value `\(\beta_1\)`.

* Shorthand: We are 95% confident that `\(\beta_1 \in [0.068,0.098]\)`

* Notice one thing in particular: `\(0 \notin [0.068,0.098]\)`!
]

--

.pull-right[
* That means that `\(\beta_1 = 0\)` is *not* a highly plausible value, given our sample of data.

* We are inclined to believe that there **is** a true relationship between `educ` and `lwage`!
]

---

# How does `R` (and others) compute those values?

* The values reported by statistical packages like R are obtained from theory under suitable assumptions.

* The theory is based on *large sample approximations*, i.e. one can show that the sampling distributions converge to suitable distributions, which we can then use the conduct tests.

* The set of assumptions needed defines the *Classical Regression Model* (CRM)

---
layout: false
class: title-slide-section-red, middle

# The Classical Regression Model



---
layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 

---

# Notation: `\(b_1 \Rightarrow \beta_1\)`

Let's talk about the small but important modifications we applied to our model:

* `\(\beta_0\)` and `\(\beta_1\)` and intercept and slope parameters

* `\(\varepsilon\)` is the error term.

We made this change to indicate that `\(b_0,b_1,e\)` are point estimates and sample errors, respectively, while `\(\beta_0,\beta_1,\varepsilon\)` are the true population parameters.

---

# Defining The CRM

1. The data are **not linearly dependent**: Each variable provides new information for the outcome, and it cannot be replicated as a linear combination of other variables.

2. The mean of the residuals conditional on `\(x\)` should be zero, `\(E[\varepsilon|x] = 0\)`. Notice that this also means that `\(Cov(\varepsilon,x) = 0\)`, i.e. that the errors and our explanatory variable(s) should be *uncorrelated*.

3. The data are drawn from a **random sample** of size `\(n\)`: observation `\((x_i,y_i)\)` comes from the exact same distribution, and is independent of observation `\((x_j,y_j)\)`, for all `\(i\neq j\)`.

4. The variance of the error term `\(\varepsilon\)` is the same for each value of `\(x\)`: `\(Var(\varepsilon|x) = \sigma^2\)`. This property is called **homoskedasticity**.

5. The error is normally distributed, i.e. `\(\varepsilon \sim \mathcal{N}(0,\sigma^2)\)`
  
  
Invoking assumption 5. in particular defines what is commonly called the *normal* linear regression model.

---

# Breaking the CRM Assumptions

.pull-left[
* No perfect collinearity. Remember `wtplus`?
    
    ```r
    library(dplyr)
    mtcars %&gt;%
      mutate(wtplus = wt + 1) %&gt;%
      lm(mpg ~ wt + wtplus, data = .)
    ```
    
    ```
    ## 
    ## Call:
    ## lm(formula = mpg ~ wt + wtplus, data = .)
    ## 
    ## Coefficients:
    ## (Intercept)           wt       wtplus  
    ##      37.285       -5.344           NA
    ```
]

.pull-right[
Conditional mean `\(E[\varepsilon|x] = 0\)`. Consider:
    `$$\text{lwage}_i = \beta_0 + \beta_1 \text{educ}_i + \varepsilon_i$$`
* Suppose there is *unobserved* ability `\(a_i\)`. High ability means higher wage.
* It *also* means school is easier, and so `\(i\)` selects into more schooling.
* Given it's *unobserved*,  `\(a_i\)` goes into the error `\(\varepsilon_i\)`
* We will attribute to `educ` part of the effect on wages that is actually *caused* by ability `\(a_i\)`!
* Our *ceteris paribus* assumption (all else equal) does not hold. 
* More examples in our book! Takeaway: if assumptions violated, theoretical inference invalid!
]

---
layout: false
class: title-slide-section-red, middle

# Simluation Based Inference



---
layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 


---

# Simluation Based Inference

.pull-left[
* We proceed as before with `infer`

* We'll generate bootstrap samples from our dataset.

* We estimate many different coefficients and look at their sampling distribution.

]

.pull-right[

```r
bootstrap_distn_slope &lt;- wage1 %&gt;% 
  specify(formula = lwage ~ educ) %&gt;%
  generate(reps = 1000, type = "bootstrap") %&gt;% 
  calculate(stat = "slope")

bootstrap_distn_slope
```

```
## # A tibble: 1,000 x 2
##    replicate   stat
##        &lt;int&gt;  &lt;dbl&gt;
##  1         1 0.0744
##  2         2 0.0887
##  3         3 0.0943
##  4         4 0.0862
##  5         5 0.0912
##  6         6 0.0870
##  7         7 0.0828
##  8         8 0.0771
##  9         9 0.0900
## 10        10 0.0817
## # â€¦ with 990 more rows
```
]

---

# Bootstrap Sampling Distribution of `\(b_1\)`



```r
visualize(bootstrap_distn_slope)
```

&lt;img src="reg_inference_files/figure-html/unnamed-chunk-8-1.svg" style="display: block; margin: auto;" /&gt;

---

# Confidence Intervals

.pull-left[

* percentile method:
    
    ```r
    percentile_ci &lt;- bootstrap_distn_slope %&gt;% 
      get_confidence_interval(type = "percentile", level = 0.95)
    percentile_ci
    ```
    
    ```
    ## # A tibble: 1 x 2
    ##   `2.5%` `97.5%`
    ##    &lt;dbl&gt;   &lt;dbl&gt;
    ## 1 0.0680  0.0987
    ```

* SE-method:
    
    ```r
    observed_slope &lt;- coef(hourly_wage)[2] # [1] is intercept
    se_ci &lt;- bootstrap_distn_slope %&gt;% 
      get_ci(level = 0.95, type = "se", point_estimate = observed_slope)
    se_ci
    ```
    
    ```
    ## # A tibble: 1 x 2
    ##    lower  upper
    ##    &lt;dbl&gt;  &lt;dbl&gt;
    ## 1 0.0673 0.0982
    ```
]


.pull-right[
* The *theoretical* bounds of the CI are are given to use in the above table.

* We had `\([0.068,0.098]\)`. 

* Let's put all three together and compare!
]


---

# Simulated vs Theoretical CIs


```r
visualize(bootstrap_distn_slope) + 
  shade_confidence_interval(endpoints = percentile_ci, fill = NULL, 
                            linetype = "solid", color = "black") + 
  shade_confidence_interval(endpoints = se_ci, fill = NULL, 
                            linetype = "dashed", color = "black") +
  shade_confidence_interval(endpoints = c(0.068,0.098), fill = NULL, 
                            linetype = "dotted", color = "black")
```


---

# Simulated vs Theoretical CIs


&lt;img src="reg_inference_files/figure-html/unnamed-chunk-12-1.svg" style="display: block; margin: auto;" /&gt;

---

# Hypothesis Test on `\(\beta_1\)`

.pull-left[
* Let's do this hypothesis test:
    `$$\begin{align}H_0:&amp; \beta_1 = 0\\H_A:&amp; \beta \neq 0\end{align}$$`
    
* If there is no relationship, then *reshuffling* or *permuting* variable `educ` should play no role. (remember the `gender` label!)

* For each of the permuted samples, we'll recompute `\(b_1\)`.

* Let's do that 1000 times over!

]


--


.pull-right[

```r
null_distn_slope &lt;- wage1 %&gt;% 
  specify(lwage ~ educ) %&gt;%
  hypothesize(null = "independence") %&gt;% 
  generate(reps = 1000, type = "permute") %&gt;% 
  calculate(stat = "slope")
```

Finally, let's visulalize the null distribution under `\(H_0: \beta_1 = 0\)`!
]

---

# Visualize `\(H_0: \beta_1 = 0\)`

.pull-left[

```r
visualize(null_distn_slope)
```

&lt;img src="reg_inference_files/figure-html/unnamed-chunk-14-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
* Notice how it's centered at 0.0! That's the most common value amongst our hyopthetical `\(b_1\)`'s in this case.

* How likely is our observed estimate `observed_slope`, i.e. 0.08 in such a universe?

* Let's get the associated pvalue!
]

---

# Adding the pvalue

.pull-left[

```r
visualize(null_distn_slope) + 
  shade_p_value(obs_stat = observed_slope, direction = "both")
```

&lt;img src="reg_inference_files/figure-html/unnamed-chunk-15-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
* Notice that we said `direction = "both"`: We have a two-sided alternative!

* Our observed slope falls *very* far outside that distribution! There seems to be almost **no** probability mass (no red shaded area visible)

* Let's confirm that and compute the p-value!

]

---

# Computing P-value from Bootstrapped Data


```r
null_distn_slope %&gt;% 
  get_p_value(obs_stat = observed_slope, direction = "both")
```

```
## # A tibble: 1 x 1
##   p_value
##     &lt;dbl&gt;
## 1       0
```

* That **is** small! ðŸ˜€

* We **reject the Null** that `\(\beta_1 = 0\)`, because the p-value 0 is smaller than any significance level `\(\alpha&gt;0\)` we could ever require. We have *extremely convincing* evidence at hand to reject the null in this case.

* We conclude that there is a **significant** relationship between `educ` and `lwage`.

* We would say:
&gt; `\(\beta_1\)` is *statistically significantly* different from zero at the `\(\alpha\)` level.

---
layout: false
class: title-slide-section-red, middle

# Illustrating Regression Uncertainty



---
layout: true

&lt;div class="my-footer"&gt;&lt;img src="../img/logo/ScPo-shield.png" style="height: 60px;"/&gt;&lt;/div&gt; 



---

# Illustrating Uncertainty

.pull-left[
* So, to answer our initial question: **yes** the slope in this picture is actually different from zero!

* This is quite encouraging. ðŸ˜‰

* We can visualize the 95% confidence interval *of the predicted line*. 

* The line could be anywhere in the grey area with 95% probability!
]

.pull-right[
&lt;img src="reg_inference_files/figure-html/unnamed-chunk-17-1.svg" style="display: block; margin: auto;" /&gt;
]



---

# [`ungeviz`](https://github.com/wilkelab/ungeviz) and `gganimate` bring you: Moving Lines!

.pull-left[
![](../img/gifs/wages.gif)

]


.pull-right[

* I split by gender to make it more interesting. ðŸ˜‰

* I take 20 bootstrap samples from the `wage1` data. You can see how different data points are included in each bootstrap sample.

* Those different points imply different regression lines.

* You should remember those moving lines when you go back one slide to understand what the shaded area means!

]



---

class: title-slide-final, middle

# THANKS

To the amazing [moderndive](https://moderndive.com/) team!

Big Thanks ðŸŽ‰ to [ungeviz](https://github.com/wilkelab/ungeviz) and ðŸŽŠ [gganimate](https://github.com/thomasp85/gganimate) for their awesome packages!

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| &lt;a href="mailto:florian.oswald@sciencespo.fr"&gt;.ScPored[&lt;i class="fa fa-paper-plane fa-fw"&gt;&lt;/i&gt;]               | florian.oswald@sciencespo.fr       |
| &lt;a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides"&gt;.ScPored[&lt;i class="fa fa-link fa-fw"&gt;&lt;/i&gt;] | Slides |
| &lt;a href="https://scpoecon.github.io/ScPoEconometrics"&gt;.ScPored[&lt;i class="fa fa-link fa-fw"&gt;&lt;/i&gt;] | Book |
| &lt;a href="http://twitter.com/ScPoEcon"&gt;.ScPored[&lt;i class="fa fa-twitter fa-fw"&gt;&lt;/i&gt;]                          | @ScPoEcon                         |
| &lt;a href="http://github.com/ScPoEcon"&gt;.ScPored[&lt;i class="fa fa-github fa-fw"&gt;&lt;/i&gt;]                          | @ScPoEcon                       |
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="../js/ru_xaringan.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
