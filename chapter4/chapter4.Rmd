---
title: "ScPoEconometrics"
subtitle: "Multiple Regression Model"
author: "Florian Oswald, Gustave Kenedi and Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---


layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(magrittr)
library(plotly)
library(reshape2)
library(haven)
library(tidyverse)
library(AER)
library(gtools)
library(countdown)

# countdown style
countdown(
  color_border              = "#d90502",
  color_text                = "black",
  color_running_background  = "#d90502",
  color_running_text        = "white",
  color_finished_background = "white",
  color_finished_text       = "#d90502",
  color_finished_border     = "#d90502"
)
```


# Short quiz on last week material

---

# Today - Multiple Regression Model

* Multiple independent variables in our model

* Interpretation for continuous and dummy variables

* Dummy variable trap

* Adjusted $R_2$

* Ommitted variable bias

* Empirical applications:

  * *Class size* and *student performance*
  
  * *Wage*, *education* and *gender* 

---

# Class size and student performance

* Let's go back to Angrist and Lavy's (1999)'s analysis of the effect of class size on student performance in Israel.

--

* With a **simple linear regression**, we found that class size was positively ***associated*** with students' scores in maths and reading.

---

# Class size and student performance: Raw relationship

```{r, echo=FALSE, fig.height=4.75, fig.width = 8}
grades = read_dta(file = "https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1")

set.seed(123)

grades %>%
ggplot(aes(x = classize, y = avgmath)) + 
  geom_point(size = 2, alpha = 0.5, position = "jitter") +
  xlim(0,45) +
  ylim(0, 100) +
  labs(
    x = "Class size",
    y = "Average score",
    title = "Mathematics") +
  theme_bw(base_size = 14)
```

---

# Class size and student performance: Raw relationship

```{r, echo=FALSE, fig.height=4.75, fig.width = 8}
set.seed(123)

grades %>%
ggplot(aes(x = classize, y = avgmath)) + 
  geom_point(size = 2, alpha = 0.5, position = "jitter") +
  xlim(0,45) +
  ylim(0, 100) +
  labs(
    x = "Class size",
    y = "Average score",
    title = "Mathematics") +
  theme_bw(base_size = 14) +
  geom_smooth(se = FALSE, method = "lm")
```

---

# Class size and student performance: Raw relationship

.pull-left[
```{r, echo=FALSE, fig.height=6}
set.seed(123)

grades %>%
ggplot(aes(x = classize, y = avgmath)) + 
  geom_point(size = 2, alpha = 0.5, position = "jitter") +
  xlim(0,45) +
  ylim(0, 100) +
  labs(
    x = "Class size",
    y = "Average score",
    title = "Mathematics") +
  theme_bw(base_size = 20) +
  geom_smooth(se = FALSE, method = "lm")
```
]

.pull-right[

```{r}
lm(avgmath ~ classize, grades)
```

* Zoom poll on interpretation?

* *Interpretation:* ***On average***, a 1-student increase in class size is ***associated*** to a `r round(lm(avgmath~classize, grades)$coefficients[2],2)` points increase in average math score.
]

---

# Class size and student performance

* Let's go back to Angrist and Lavy's (1999)'s analysis of the effect of class size on student performance in Israel.

* With a **simple linear regression**, we found that class size was positively ***associated*** with students' scores in maths and reading.

* This is intuitively unexpected, and contrasts with the simple results from the *STAR* randomized experiment.

--

* Could it be that some other variable may be related to class size ***as well as*** students' performance?

* In particular, we mentioned the **location effect**: large classes may be more common in wealthier and bigger cities, while small classes may be more likely in poorer rural areas.

--

* Let's investigate this hypothesis.

---

# Class size and student performance: Confounders

.pull-left[
Link between **class size** and **the share of student who come from `disadvantaged` background** in the class.

```{r, echo = FALSE, fig.height=4.75, fig.width = 8}
ggplot(grades, aes(x = disadvantaged, y = classize)) + 
  geom_point(size = 2, alpha = 0.4, position = "jitter") +
  xlim(0,100) +
  ylim(0,45) +
  labs(
    x = "% class coming from disadvantaged background",
    y = "Class size",
    title = "Relationship with class size") +
  theme_bw(base_size = 20) +
  geom_smooth(se = FALSE, method = "lm")
```

`r emo::ji("point_right")` On average, there is a greater % of disadvantaged students in smaller classes.
]

--

.pull-right[
Link between **average math score** and **the share of student who come from `disadvantaged` background** in the class.

```{r, echo = FALSE, fig.height=4.75, fig.width = 8}
ggplot(grades, aes(x = disadvantaged, y = avgmath)) + 
  geom_point(size = 2, alpha = 0.4, position = "jitter") +
  xlim(0,100) +
  ylim(0,100) +
  labs(
    x = "% class coming from disadvantaged background",
    y = "Average math score",
    title = "Relationship with average math score") +
  theme_bw(base_size = 20) +
  geom_smooth(se = FALSE, method = "lm")
```

`r emo::ji("point_right")` On average, the greater the % of students coming from a disadvantaged background, the lower the average math score.

]
---

# Class size and student performance: Multiple regression

* Suppose we want to know the effect of class size on average math scores, ***controlling for*** the fact that there is a negative relationship between the % of disadvantaged students and class size **AND** average math score.

--

* To do so, we have to include both `classize` and `disadvantaged` variables as *regressors* in the regression.

--

* As such we can obtain an estimate of the effect of class size on average math score, ** *purged* of the effect of the disadvantaged variable**.

--

* The model we want to estimate becomes:

$$
 \textrm{mathscore}_i = b_0 + b_1 \textrm{classize}_i + b_2 \textrm{disadvantaged}_i + e_i
$$
--

* This is ***multiple regression***! We will estimate this model in a few slides. Let's formalize what we have seen so far.

---

# Multiple Regression's Purpose

* Recall from two weeks ago, the **Simple Linear Model** can be written as

$$y_i = b_0 + b_1 x_i + e_i,$$

where $y_i$ is the ***dependent variable*** and $x_i$ is the ***independent variable***.

--

`r emo::ji("warning")` Unless all other factors affecting $y_i$ are uncorrelated with $x_i$, $b_1$ **cannot be interpreted as a causal effect**.

--

We need to **enrich the model** and take into account factors that are simultaneously related to $y_i$ **and** $x_i$.

---

# Mutiple Regression Model

The expanded model can be written as:

$$y_i = b_0 + b_1 x_{1,i} + b_2 x_{2,i}  + b_3 x_{3,i} + \dots + b_k x_{k,i} + e_i,$$

where $x_1$, $x_2$, ..., $x_k$ are $k$ regressors, and $b_1$, $b_2$, ..., $b_k$ are the associated $k$ coefficients.

--

***Estimation***: We obtain the values for $(b_0, b_1, b_2, ..., b_k)$ in the same way as before, using **OLS**. 

* $(b_0^{OLS}, b_1^{OLS}, b_2^{OLS}, ..., b_k^{OLS})$ are the values that minimize the **Sum of Squared Residuals**. 
  
* That is they minimize
$$
\begin{align}
\sum_{i}{e_i^2} &= \sum_{i}{(y_i - \hat{y_i})^2} \\
&= \sum_{i}{[y_i - (b_0 + b_1 x_{1,i} + b_2 x_{2,i}  + b_3 x_{3,i} + \dots + b_k x_{k,i})]^2}
\end{align}
$$

---

# Multiple Regression Model: Interpretation

For now assume both the dependent variable $(y_i)$ and the independent variables $(x_k)$ are numeric.

> Intercept $(b_0)$: **The predicted value of $y$ $(\widehat{y})$ if all the regressors $(x_1, x_2, x_3,...)$ are equal to 0.**

--

> Slope $(b_k)$: **The predicted change, on average, in the value of $y$ *associated* to a one-unit increase in $x_k$...** <br/>
> $\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad$ **... keeping all the other regressors constant!**

--

* Notice that the *keeping all the other regressors constant* is the only part that changes compared to SLM.

* In other words, you are considering the indidivudal effect of the variable $x_k$ on $y$ **in isolation** of the effect that the other regressors might have on $y$.

--

* **Link with causal inference**: Only the regressors included in the model are held constant, those that are not in the model can still vary and "bias" your estimates.

---

# Ommitted variable bias (OVB)

* ***ommitted variable bias***: Omitting important control variables from the regression model

* This renders the coefficient for your regressor of interest unreliable.

--

* Recall our previous example of class size and disadvantaged students. Not taking into account the `disadvantaged` variable led to a OVB for our class size estimate.

--

* The formula for OVB is:

$$\textrm{OVB} = \{ \textrm{Relationship between } disadvantaged_i \textrm{ and } classize_i \} \\ * \{ \textrm{Effect of } disadvantaged_i \textrm{ in multiple regression} \}$$

--

* From this formula you obtain both the *mangitude* of OVB and its *sign* (positive/negative).

--

* The relationship between `disadvantaged` and `classize` is negative, and the effect of `disadvantaged` in the multiple regression is negative, therefore the OVB is positive. In other words, ommitting `disadvantaged` from the regression leads to an inflated coefficient for class size.

---

# Multiple Regression with `R`

* Very similar to simple linear regression:

```{r, echo = TRUE, eval = FALSE}
lm(formula = dependent variable ~  independent variable 1 + independent variable 2 + ...,
   data = data.frame containing the data)
```

--

## Class size and student performance: Multiple regression

Let's estimate the model from earlier on by OLS: $\textrm{mathscore}_i = b_0 + b_1 \textrm{classize}_i + b_2 \textrm{disadvantaged}_i + e_i$
 
```{r}
lm(avgmath ~ classize + disadvantaged, grades)
```


---

# Class size and student performance: Multiple regression

```{r, echo = FALSE, eval=TRUE}
lm(avgmath ~ classize + disadvantaged, grades)
```

***Questions*** => zoom or wooclap poll!

1. How do you interpret each of these coefficients?
1. How do you explain the change in the `classize` coefficient compared to the SLM case?

---

# Class size and student performance: Multiple regression

```{r, echo = FALSE, eval=TRUE}
lm(avgmath ~ classize + disadvantaged, grades)
```

***Answers***

1\. How do you interpret each of these coefficients?

* $b_0$ = 69.9: When `class size` and `disadvantaged` are set to 0, the *predicted* value of the average math score is 69.9.

--

* $b_1$ = 0.07: Keeping the share of *disadvantaged students* constant in the class, a 1-student increase in class size is ***associated, on average,*** with a 0.07 point increase in average math score.

--

* $b_2$ = - 0.34: Keeping the *class size* constant, a 1-percentage point increase in the share of *disadvantaged students* is ***associated, on average,*** with a 0.34 point decrease in average math score.

---

# Class size and student performance: Multiple regression

```{r, echo = FALSE, eval=TRUE}
lm(avgmath ~ classize + disadvantaged, grades)
```

***Answers***

2\. How do you explain the change in the `classize` coefficient compared to the SLM case?

* $b_1$ decreases when the `disadvantaged` variable is taken into account. This was expected since part of the positive effect of class size was partly due to the smaller share of disadvantaged students in bigger classes.
  
---

# Multiple Regression in 3D => Useful?


```{r plane3D-reg, echo=FALSE, message=FALSE, warning=FALSE}
# linear fit
fit <- lm(avgmath ~ classize + disadvantaged, grades)
 
to_plot_x <- range(grades$classize)
to_plot_y <- range(grades$disadvantaged)

df <- data.frame(classize = rep(to_plot_x, 2),
           disadvantaged = rep(to_plot_y, each = 2))

df["pred"] <- predict.lm(fit, df, se.fit = F)

surf <- acast(df,disadvantaged ~ classize)

color <- rep(0, length(df))
grades %>%
  plot_ly(width = 5, height = 5, tickfont = list(size =7)) %>%
  add_markers(x = ~classize, y = ~disadvantaged, z = ~avgmath,
              name = "1 dot = 1 class",opacity = .6, 
              marker=list(color = 'blue',
                          size = 1.5, 
                          hoverinfo="skip")) %>%
  add_surface(x = to_plot_x, y = to_plot_y, z = ~surf,
              inherit = F,
              name = "3D regression plane",
              opacity = .7, cauto = F,
              colorscale = list(c(0, 1), c("red", "yellow"))) %>%
   layout(xaxis = list(tickfont = list(size =7)), yaxis = list(tickfont = list(size =7)))
```


---

class:inverse

# Task 1

`r countdown(minutes = 10, top = 0)`

Let's analyse the regression results using **reading** score as the dependent variable.

1. Load the data from [here](https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1) using the `read_dta()` function from the `haven` package. Assign it to an object `grades`.

1. Regress `avgverb` on `classize` and `disadvantaged`. Assign the output to a new object `reg`. 

1. Look at the coefficients of this regression by running `reg$coefficients`. How do they compare with the math score regression coefficients?

1. What are the other available variables that we may add in the regression? 
  * Run the regression with all these variables and assign it to `reg_full`.
  * Look at the coefficients.
  * Discuss all coefficients: sign and magnitude.

---

# No Perfect Collinearity

There is one condition to satisfy to add regressors to the model:

> Any additional variable needs to add **at least *some* new information**.

--

In other words, regressors **cannot be perfectly collinear**, i.e. not linear combinations of one another:

  $$ x_2 \neq ax_1 + b $$ 

--

Even if not perfectly correlated, the individual effects of highly correlated regressors are hard to disantangle.

--

Note that this implies that the number of observations has to be greater than the number of independent variables.

---

# Dummy Variable Trap

add stuff on dummy variable

explain interpretation of categorical variables

---

class: inverse

# Task 2

`r countdown(minutes = 7, top = 0)`

Still need to think about this perfect collinearity issue? Let's run a regression where there is perfect linear dependence between regressors.

1. Load the *STAR* data from [here]("https://www.dropbox.com/s/bf1fog8yasw3wjj/star_data.csv?dl=1") and assign it to an object called `star_df`. Keep only cases with no `NA`s with the following code:  
`star_df <- star_df[complete.cases(star_df),]`  
Keep only second graders and small and regular class groups.

1. Create a dummy variable `small` equal to `TRUE` if students are in a small class and `FALSE` if they are in a regular class. In the same way, create a dummy variable `regular` equal to `TRUE` if students are in a regular class and to `FALSE` if they are in a small class. Clearly these two variables are perfectly collinear.

1. Regress `small` on `math`. What is the average predicted `math` score of students in a small class?

1. Regress `regular` on `math`. What is the average predicted `math` score of students in a regular class?

1. Regress `small` and `regular` on `math`. What do you notice? Can you explain why?

---

# Adjusted $R^2$

* Not of great importance but because it is so widely reported you just need to know it.

--

* By construction, $R^2$ will always increase when a new regressor is added to the regression.

--

* The *adjusted $R^2$* imposes a penalty for adding regressors to the model. The details are not crucial as in the vast majority of cases the $R^2$ and the adjusted $R^2$ are pretty similar.

---


class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# SEE YOU NEXT WEEK!


|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

