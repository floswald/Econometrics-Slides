---
title: "ScPoEconometrics"
subtitle: "Multiple regression Model"
author: "Florian Oswald, Gustave Kenedi and Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---


layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(magrittr)
library(plotly)
library(reshape2)
library(haven)
library(tidyverse)
```


# Recap from last week and today Plan 

* Introduction to causality

  * Causality versus correlation
  * The Potential Outcome Framework a.k.a. Rubin's Causal Model
  * Randomized controlled trials (RCTs)

--

## Today - Multiple Linear Regression Model

.pull-left[
* Multiple regression model
  * Interpretation of coefficients 

* Variations from the baseline model
  * Standardized regression
  * Log models 
  * Interacting variables 
  
]
--
.pull-right[
* **Applications** 

  * Class size and student achievments
  
  * Wage, education and gender 
]

---

# Why making Multiple Regression

* We already introduced what a **Simple Linear Model** is ...

$$y_i = b_0 + b_1 x_i + e_i$$

For example : 

$$\textrm{Score_i} = b_0 + b_1 \textrm{classsize}_i + e_i$$

* ... and how to estimate $b_0$ and $b_1$ from the data, that is making an **OLS** regression. 

* Most of the time such a simple approach will **prevent you from making causal claim** about $b_1$. 

--

* So, what if we can **enrich the model** and take into account more factors? 

  * For example, what if we also **control for** (some) schools and student characteristics (share of disaventaged people, school size,...)?
  
* That's what **Multiple Regression** allows us to do!

---

# Mutiple Regression model 

Let's consider the following model

$$y_i = b_0 + b_1 x_{i,1} + b_2x_{i,2} + b_3x_{i,3} + e_i$$
* It's a multiple linear regression model with $3$ regressors. It can easily be generalized to more.

  * N.B. : we now also have an index for the regressors: k=1,2,3,...

--

* **Estimation** : We get the values for $(b_0, b_1, b_2, b_3)$ in the same way as before, with **OLS** estimation. 

  * $(b_0^{OLS}, b_1^{OLS}, b_2^{OLS}, b_3^{OLS})$ are the values that minimize the **Sum of Squared Residuals**. 
  
  *  That is the $b$'s which minimize $\sum_{i}{e_i^2} = \sum_{i}{(y_i - \hat{y_i})^2}$ 
  
  where $\hat{y_i} = b_0 + b_1 x_{i,1} + b_2x_{i,2} + b_3x_{i,3}$

---

# Interpreting coefficients in Multiple Regression Model 

How do we interpret each individual coefficient in a multiple regression model: 

> Intercept $(b_0)$: **The predicted value of $y$ $(\widehat{y})$ if all the regressors ( $x_1$, $x_2$, $x_3$,...) are equal to 0. **

> Slope $(b_k)$: **The predicted change, on average, in the value of $y$ *associated* to a one-unit increase in $x_k$...** <br/>
> $\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad$ **... keeping all the other regressors constant** !

* Notice that the *keeping all the other regressors constant* is the only part that changes compared to SLM. 

* It means that you are considering the indidivudal effect of the variable $k$ on $y$ **in isolation** of the effect that the other regressors might have. 

* In math we talk it refers to the notion of *partial derivative* :  $b_k = \frac{\partial y}{\partial x_k}$

--

* **Link with causal inference**: Only the regressors included in the model are hold constant, those who are not in the models can still vary and bias your estimates.  

---

# Class size and student achievment

* Let's reconsider the effect of class size on student achievment in a multiple regression framework.

* Remember fom the previous class that we found : 
  * .small[A predicted **increase** in test scores when class size increases with a **simple linear regression**.] 
  * .small[A predicted **decrease** in test scores when class size increases with the **STAR experiment** data.]

.pull-left[

```{r, echo=FALSE, fig.height=4}
grades = read_dta(file = "https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1")

ggplot(grades, aes(x = classize, y = avgmath)) + 
    geom_point(size = 2, alpha = 0.4) +
  geom_jitter(alpha = 0.4) +
    xlim(0,45) +
    ylim(50, 100) +
    labs(
      x = "Class size",
      y = "Average score",
      title = "Mathematics test score") +
    theme_bw(base_size = 18) +
  geom_smooth(se = FALSE, method = "lm")
```

]

.pull-right[

<br/>

* On average, an increase in class size (by one student) is associated to an increase in math test score by `r round(lm(avgmath~classize, grades)$coefficients[2],2)`. 
]

---

# Class size and student achievment

* .small[But we may suspect that (a lot of) other things also change when considering classes of smaller / bigger size.]

* .small[In particular, we mentioned the **location effect** : Large classes may be more common in wealthier and bigger cities, while small classes may be more likely in poorer rural areas.]

* .small[Let's look at the link between class size and **the share of student who come from disadvantaged background** in the class (`disadvantaged` variable).]

```{r, echo = FALSE, fig.height=3, fig.width=5}
ggplot(grades, aes(x = classize, y = disadvantaged)) + 
    geom_point(size = 2, alpha = 0.4) +
  geom_jitter(alpha = 0.4) +
    xlim(0,45) +
    ylim(0,85) +
    labs(
      x = "Class size",
      y = "Average",
      title = "Percent of class coming from disadvantaged background") +
    theme_bw(base_size = 10) +
  geom_smooth(se = FALSE, method = "lm")
```

---

# Class size and student achievment

* On average, small size classes concentrate **more students from disadvantaged background**

* And in the same time, we observe that students from disadvantaged background **also have lower scores on average**. 

```{r, echo = FALSE, fig.height=3.5, fig.width=6}
ggplot(grades, aes(x = disadvantaged, y = avgmath)) + 
    geom_point(size = 2, alpha = 0.4) +
  geom_jitter(alpha = 0.4) +
    xlim(0,80) +
    ylim(25, 100) +
    labs(
      x = "Percent of class coming from disadvantaged background",
      y = "Average Score",
      title = "Mathematics") +
    theme_bw(base_size = 15) +
  geom_smooth(se = FALSE, method = "lm")
```


---

# Class size and student achievment

* Not taking into account the `disadvantaged` variable would then lead to **omitted variable bias** in our class size estimate. 

* It is when you omit a variable that **causes your outcome and that is correlated with your regressor(s)**.

  * .small[N.B. : Here we did not actually proove that more `disadvantaged` students *causes* the grades to be lower but it seems quite reasonable.]
  
--

* So, if we want to isolate the effect of class size from the effect of coming from a more/less disadvantaged background...

* ... we have to include both variables in our regression.

  * Then, we will be able to get an estimate of the effect of class size on grades, ** *purged* from the effect of the disadvantaged variable**.

---

# Multiple regression in practice 

Let's regress the math test score on class size **and** the share of disadvantaged student per class. 

$$ \textrm{math score}_i = b_0 + b_1 \textrm{class size}_i + b_2 \textrm{disadvantaged}_i + e_i$$

```{r,echo=FALSE}
grades_avg_cs = grades %>%
  group_by(classize) %>%
  summarise(avgmath_cs = mean(avgmath),
            avgverb_cs = mean(avgverb), 
            avgdisad_cs = mean(disadvantaged))
```

* The **R command** will be exactly the same as for **S**imple **L**inear **R**egressions

  * except here your will put several regressors on the right hand side seprarated by `+`.
  
* Here is the command for the above equation: 
```{r, eval = FALSE}
lm(avgmath ~ classize + disadvantaged, grades)
```

--

.pull-left[
* And these are the estimates we get :   
```{r, echo=FALSE, eval=TRUE, out.height=3}
lm(avgmath ~ classize + disadvantaged, grades)$coefficients
```
]

--

.pull-right[
* You can compare with the class size estimate we got from the SLR :   
```{r, echo=FALSE, eval=TRUE, out.height=3}
lm(avgmath ~ classize, grades)$coefficients
```
]

---

# Multiple regression in practice 

```{r, echo=FALSE, eval=TRUE, out.height=3}
lm(avgmath ~ classize + disadvantaged, grades)$coefficients
```

* **Question** 

  * .small[How do you interpret each of these coefficients?]
  * .small[How do you explain the evolution of `classize` estimate compared to the SLM case?] 

---

# Multiple regression in practice 

```{r, echo=FALSE, eval=TRUE, out.height=3}
lm(avgmath ~ classize + disadvantaged, grades)$coefficients
```

**Answers** 

1) How do you interpret each of these coefficients?

* $b_0$ = 69.9 : .small[When `class size` and `disadvantaged` are set to 0, the *predicted* value of the math score is 69.9]

* $b_1$ = 0.07 : .small[Keeping the share of *disadvantaged students* constant in the class, if we increase the class size by 1 student, the math score will increase (on average) by 0.07 (point).]

* $b_2$ = - 0.34 : .small[Keeping the *class size* constant, if we increase the share of *disadvantaged students* by **one percentage point**, the math score will decrease (on average) by 0.34 (point).]

--

2) How do you explain the evolution of `classize` estimate compared to the SLM case?

* $b_1$ .small[decreases by taking into account the `disadvantaged` variable. It was expected since part of this positive effect is actually due to the smaller share of disadvantaged students in bigger classes.]
  
---

# Multiple regression in 3D

Multiple Regression - a plane in 3D. 


```{r plane3D-reg,fig.width=8,fig.height=4, echo=FALSE, message=FALSE, warning=FALSE}
# linear fit
fit <- lm(avgmath ~ classize + disadvantaged, grades)
 
to_plot_x <- range(grades$classize)
to_plot_y <- range(grades$disadvantaged)

df <- data.frame(classize = rep(to_plot_x, 2),
           disadvantaged = rep(to_plot_y, each = 2))

df["pred"] <- predict.lm(fit, df, se.fit = F)

surf <- acast(df,disadvantaged ~ classize)

color <- rep(0, length(df))
grades %>%
  plot_ly(width = 5, height = 5, tickfont = list(size =7)) %>%
  add_markers(x = ~classize, y = ~disadvantaged, z = ~avgmath,
              name = "1 dot = 1 class",opacity = .6, 
              marker=list(color = 'blue',
                          size = 1.5, 
                          hoverinfo="skip")) %>%
  add_surface(x = to_plot_x, y = to_plot_y, z = ~surf,
              inherit = F,
              name = "3D regression plane",
              opacity = .7, cauto = F,
              colorscale = list(c(0, 1), c("red", "yellow"))) %>%
   layout(xaxis = list(tickfont = list(size =7)), yaxis = list(tickfont = list(size =7)))
```


---

class:inverse

# Task 1 (10 min)

Let's see how it goes for the **reading** score. 

First of all, load the data from [here](https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1) using the `read_dta()` from the `haven` package. Assign it to a dataframe called `grades`. 

1) Regress the `avgverb` (reading) test score on `classize` and `disadvantaged` and assign it to `reg`. 

2) Look at the coefficients of this regression by running `reg$coefficients`. How do they compare with the math score regression? 
```{r, echo = FALSE, eval=FALSE}
lm(avgverb ~ classize + disadvantaged, grades)$coefficients
```

3) What are the other available variables that we may add in the regression? 
  * Run the regression with all these variables and assign it to `reg_full`. 
  * Look at the coefficients.
  * Discuss all estimates, both their sign and magnitude. 
```{r, echo = FALSE, eval=FALSE}
lm(avgverb ~ classize + disadvantaged + school_enrollment + female + religious, grades)$coefficients
```

---

# Multicollinearity issue

There is a rule to follow when adding variables in your model. 

* Intuitively, we want to add what we thinks is important to explain the variations in our outcome. 

* In practice, there is a technical restriction : any additional variable needs to add **at least *some* new information**. 

  * We often refer to this condition as the **Rank condition** : we need our matrix of $X$s (our regressors) to be of [full rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra)). 

* In other words, regressors **must not be perfectly collinear**, i.e. not perfectly linearly dependent.

  $$ x_2 = ax_1 + b $$ 

* N.B.: Even if not perfectly correlated, the individual effects of highly correlated regressors are hard to disantangle. 

---

# Multicollinearity : example

Let's consider $\textrm{is.female}$ and $\textrm{is.male}$ are perfectly colinear, we have : $$\textrm{is.male = 1 - is.female}$$

* We don't get any additional info from $\textrm{is.male}_i$ about $i$ if we already know the value of $\textrm{is.female}_i$. 

* Let's write our standard regression model including both variables 

$$ y = b_0 + b_1 \textrm{is.female} + b_2 \textrm{is.male} + e $$ 
So we have 


$$\begin{align}
y_i &= b_0 + b_1 \textrm{is.female} + b_2 (1 - \textrm{is.female}) + e \\
                    &= b_0 + b_2 + (b_1 - b_2) \textrm{is.female} +e 
  \end{align}$$

* We cannot tell $b1$ and $b_2$ apart! That's an *identification problem*.

* We cannot solve the the minimization problem of the SSR! That's a *numerical problem*.

---

class: inverse

# Task 2 (5 min)

.small[Still need to think about this multicollinearity issue? Let's try to run a regression when there is a perfect linear dependence between them.]

1) .small[First, generate some toy data with the following code and make sure you understand each piece of code.]

```{r,eval=FALSE}
set.seed(2)
toy_data = data.frame(wage = rnorm(10,1000,100), 
                      is.female = sample(c(0,1),10,replace = T))
toy_data$is.male = 1 - toy_data$is.female
```

2) .small[Run the regression of `wage` on both `is.female` and `is.male`. What happens?] 

3) .small[Run the regression of `wage` on `is.male` only. What is the average predicated `wage` for women?] 

4) .small[Run the regression of `wage` on `is.female` only. What is the average predicated `wage` for women again?]

5) .small[Redo 3) and 4) but computing the predicted wage of men.] 

---

# Variations from the baseline model 

Facing different datasets and different relationships between the variables of interest, you may need to move away from the baseline model.  

* We will focus on 3 important variations : 

  * **Standardized regressions**

  * **Log models**

  * Regressors **interactions**


* In each case,  the way we estimate these coefficients does not change. 

* However, the way we interpret our coefficients $(b_1, b_2,..., b_k)$ does change. 

---

# Standardizing regression

Let's define what *standardizing* a variable means. 

> **Standardizing** a variable $z$  means to *demean* the variable and to divide the demeaned value by its own standard deviation

$$ z_i^{stand} = \frac{z_i - \bar z}{\sigma(z)}$$ 
where $\sigma(z)$ is the standard deviation of $z$, i.e. $\sigma(z) = \sqrt{\textrm{VAR}(z)}$.

* Why would we do that in the first place? i.e. standardizing $Y$ and / or the $X$s 

--

* Intuitively, standardizing is about **putting things on the same scale** so we can compare between each other. 
  
* In our class size / student achievment case, it will help to interpret : 

  * The **magnitude** of the effects 
  * The **relative importance of each variable**

---

# Standardizing regression : interpretation

* If dependant variable $y$ is standardized

  * By definition, $b_k$ is measuring the predicted change in ** $y^{stand}$ **  associated with a one unit increase in $x_k$. 
  
  * If $y^{stand}$ increases by one, it means that $y$ increases by one standard deviation, so $b_k$ is measuring the change in $y$ **as a share of its own standard deviation**.
  
--

* If the regressor $X_k$ is standardized

  * By definition, $b_k$ is measuring the predicted change in $Y$ associated with a one unit increase in ** $x_k^{stand}$ **. 
  
  * If $x_k^{stand}$ increase by one unit, it means that $x_k$ increases by one standard deviation. So $b_k$ is measuring the predicted change in $y$ **associated with an increase in $x_k$ by one standard deviation.**. 
  
---

class:inverse

# Task 3 (7min)

.small[Let's take back our `grades` dataset. This are the estimates we got from regressing the math test score on the full set of regressors.] 

```{r echo = FALSE}
reg5 = lm(avgmath~classize + disadvantaged + school_enrollment + religious + female, data = grades)
reg5$coefficients
```

.small[1) Add the standardized math score as the `avgmath_stand` variable in your dataset. You can use the `standardie()` function from the `jtools` package or do it by hand with base R.]

.small[2) Run the full regression for the standardized math test score. Interpret the coefficient and their magnitude.]

.small[3) If you were to pick the most influencial variable on the math score, what would it be?]

.small[4) Add the standardized value of each *continous* regressor as a new variable called `*<regressor>*_stand`] . 
  * .small[Why do we prefer not to standardize the `religious` variable?] 
  
.small[5) Reegress `avgmath_stand` on the full set of standardized regressors and `religious`. Discuss the relative influence of the regressors.]
---

# Log models 

* The specification we have seen so far can be called **level-level** specification. Both the dependent and the independent variables are measured in level.

  * This *level* can be: euros, years, number of students,... and even percentage.

* Taking the log of the dependent and / or the independent variable leads us to define 3 other type of regression :

  * **Log - level**: $\quad \textrm{log}(y_i) = b_0 + b_1 x_{i,1} + ... + e$

  * **Level - log**: $\quad \textrm{y}_i = b_0 + b_1 log(x_{i,1}) + ... + e$

  * **Log - log**: $\quad \textrm{log}(y_i) = b_0 + b_1 log(x_{i,1}) + ... + e$

* We will see how the **interpretation** changes and **why** to use these kind of models. 

---

# Interpretating log models

Here is a table that summarise how to interpret regressor coefficient in each case:  

|    Model           | Equation  |  Interpretation of $b_1$            |
|--------------------|:---------:|:-----------------------------------:|
| Level - Level | $y = b_0 + b_1 x_{1} + e$ | .small[**One unit** increase in x is associated with <br/>] $b_1$ .small[**unit change** in y]  |
| Log - Level | $\textrm{log}(y) = b_0 + b_1 x_{1} + e$ | .small[**One unit** increase in x is associated with <br/> ] $b_1$ .small[ **percent change** in y]  |
| Level - Log | $\textrm{y} = b_0 + b_1 log(x_{1})  + e$ | .small[**One percent** increase in x is associated with<br/>] $b_1/100$ .small[**unit change** in y] |
| Log - Log  |  $\textrm{log}(y) = b_0 + b_1 log(x_{1}) + e$ | .small[**One percent** increase in x is associated with<br/>] $b_1$ .small[**percent change** in y]  |


* It looks like cooking recipes but of course it can be derived with (basic) algebra. 

---

# When do we use log models?

There are various reasons for which we would want to use log models

* Account for **non linearity** in the relationship between $y$ and $x$.

* Log-log models allow us to interpret coefficients as <a href="https://en.wikipedia.org/wiki/Elasticity_(economics)">elasticities</a> which have a central role in economic theory.

* Limit the influence of **outliers**. 
  * It comes from the concavity of the *log* function. 

---

# Interacting regressors

* We interact two regressors when we think the effect of one depends on the value of the other. 

  * In our next application, we'll let the return to education on wage vary by gender.
  
* In practice, if we interact $x_1$ and $x_2$, we would write our model like this : 

$$y_i =  b_0 + b_1 x_{i,1} + b_2 x_{i,2} + b_{1,2}x_{i,1}*x_{i,2} + ... + e$$

* The interpretation of $b_1$, $b_2$, and $b_{1,2}$ will depend on the type of $x_1$ and $x_2$. 
  
* Let's focus on the cases where one regressor is a dummy / categorical variable and the other is continuous.

* It will give you the intuition for the other cases : 

  * Both regresors are dummies / categorical variables

  * Both regresors are continous
  
---

# Interacting regressors

* Let's illustrate with the case of **doctors visits**. 
  *  .small[We use the `DoctorVisits` dataset from the `AER` package contains [cross-section data](https://en.wikipedia.org/wiki/Cross-sectional_data) originating from the 1977–1978 Australian Health Survey]

* In particular we will focus on the role of `gender` and `age` on the number of doctor `visits` in the past two weeks. 

* Our regression model is 

$$ \textrm{visits_i} = b_0 + b_1 \textrm{is.female}_i + b_2 \textrm{age}_i + b_3 \textrm{is.female}_i * \textrm{age}_i + e$$ 

* What is the predicted number of visits for 

  * A woman of age 30: $\hat{\textrm{visits}_i} = b_0 + b_1 + b_2*30 + b_3 * 30$
  * A man of age 30: $\hat{\textrm{visits}_{i'}} = b_0 + b_2*30$
  * The difference in the prediction between a man and woman of age 30 is then equal to: $b_1 + b_3*30$

---

# Interacting regressors

Running the regression we obtain 

```{r, echo = FALSE, eval = TRUE}
library(jtools)
data("DoctorVisits")
df = DoctorVisits
df$age = df$age*100
df$is.female = 1*(df$gender=="female")
round(lm(visits~ is.female + age + is.female*age, df)$coefficients,3)
```


* **Interpretation**

--

  * Thanks to our interaction term, we let the difference in the number of visits between men and women vary with age.
  
--
  
  * The coefficient assocciated to `is.female` is equal to 0.179 which means that we observe more doctor visits among *relatively young women.* 

--

  * The coefficient assocciated to `is.female:age` being negative it means this differential will *decrease with age*. 
  
---

# Interacting regressors

Here is a visualization of our model prediction

```{r,echo=F, fig.height=4, fig.width=7}
ggplot(df, aes(x = age, y = visits, group = gender, color = gender)) +
    geom_smooth(method = "lm", se = F) +
    theme_bw() +
  labs(title = "Number of visits in the past 2 weeks")
```


---

# Wages, education and gender

* Let's use these new *tools* to investigate the relationship between wages, education and gender. 

* We will use data from the [**C**urrent **P**opulation **S**urvey](https://www.census.gov/programs-surveys/cps.html). 
  * .small[This is the U.S. Government's monthly survey of unemployment and labor force participation.] 
  * .small[We'll use a sample of 534 individuals from the 1985 CPS available in the `AER` package.]

--

* Let's first have a look at the distribution of wage broken down by education and gender 

.pull-left[
```{r, echo = FALSE, fig.height=2.5,fig.width=3.5}
data("CPS1985")
cps = CPS1985
cps$education_gp = quantcut(cps$education,3)
cps$log_wage = log(cps$wage)
  
ggplot(cps, aes(x = gender, y = wage, fill = gender)) + 
    geom_boxplot() + 
    theme_bw() + 
    labs(title = "Boxplot: Wage ~ gender")+
    guides(fill=FALSE)
```
]

.pull-right[
```{r, echo = FALSE, fig.height=2.5,fig.width=4.5}
ggplot(cps, aes(x = education_gp, y = wage, fill = education_gp)) + 
    geom_boxplot() + 
    theme_bw() + 
    labs(title = "Boxplot: Wage ~ education group (in years)")+
    guides(fill=FALSE)
```

]

---

# Wages, education and gender

* So, wages are higher for men and higher educated people, well, nothing really new. 

* But, are the return to education the same for women and men? 

* In other words, how the gender gap evolves when considering different levels of education?  

  * Do you have any prior about this?

--

.pull-left[
```{r, echo = FALSE, fig.height=2.8,fig.width=5}
ggplot(cps, aes(x = gender, y = wage, fill = gender)) + 
    geom_boxplot() + 
    facet_wrap(~education_gp, ) +
    theme_bw() + 
    labs(title = "Education group (in years)")+
    guides(fill=FALSE)
```
]

--

.pull-right[
<br/>

* We do observe a gender gap for each education group

* But it's not really clear if/how it varies with education

* Let's test it with a regression!  

]

---

class:inverse

# Task 4 (10 min)

Load the data `CPS1985` from the `AER` package

* Look at the `help` to get the definition of each variable: `?CPS1985`

  * We don't know if people are working part time or full time, does it matter here? 

* Add the`log_wage` variable (log of the `wage`)

* Regress the `log_wage` on `gender` and `education`, save it as `reg1`.
  
  * Interpret each coefficient

* Regress the `log_wage` on `gender`, `education` and their interaction `gender*education`, save it as `reg2`.
  * Do the results confirm our previous guess?

* Add all the other regressors available in our data and run a new regression saved as `reg3`.

---

# Wage, Gender and Education

```{r,echo=F, fig.height=2.75, fig.width=5}
ggplot(cps, aes(x = education, y = log_wage, group = gender, color = gender)) +
    geom_point(alpha = 0.2) + 
    geom_smooth(method = "lm", se = F) +
    theme_bw() +
    ylim(0,4)
```

* We provided evidence that the average wage gap is decreasing with education... 

* ...even after controling for several factors like *experience*, *occupation*, *sector*, ...



---

# Teaser for next chapters 

* You may have noticed that since the beginning we always work with **samples** drawn from the overall population. 

* Each time, imagine we could draw another sample from population. 

  * Would we obtain the same results? 
  
  * In other words, how confident can we be that our estimates (sign, magnitude) are not just driven by hazard. 

* The next chapters will answwer those kind of questions:

  * We'll present the notion of **sampling**
  
  * Understand what **statistical inference** is and how to do it. 
  


---


class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# SEE YOU NEXT WEEK


|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

