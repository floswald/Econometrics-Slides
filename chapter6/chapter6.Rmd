---
title: "ScPoEconometrics"
subtitle: "Sampling"
author: "Florian Oswald, Gustave Kenedi and Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(magrittr)

```

# Recap from last week

--

## Today<sup>1</sup>

.footnote[
[1]: This lecture is very heavily based on the wonderful Sampling chapter of ModernDive]

* bla 1

* bla 2

---

# What's the proportion of green Fusilli?

.center[
```{r, echo = FALSE, out.width = "600px"}
knitr::include_graphics("../img/photos/pasta1.JPG")
```
]

--

We could count every green pasta but that would be tedious! `r emo::ji("weary")`

---

# Full Census is Costly


.center[
```{r, echo = FALSE, out.width = "600px"}
knitr::include_graphics("../img/photos/pasta3.JPG")
```
]

--

What else could we do?

---

# Sampling

.pull-left[
* Let's take a sample of $20$ fusilli.

* We made sure to selected fusilli at **random**.

* Here is what we found.

Color | Count | Proportion
:------:|:------:|:--------:
Red   |  7        |  0.35
Green   |  5     |   0.25
White   |  8    |     0.4

* 0.34 can be thought of as our guess of the proportion of green pasta in the entire bowl.
]

.pull-right[
<div><img src="../img/photos/pasta5.JPG"?></div>
]

---

# Sampling Variation


* What would happen if we took a *new* sample (putting the 20 previous pasta back in the bowl)? Would we also get 5 *greens* as before?

--

* What if we repeated this activity multiple times?

--

* Probably not. The samples will vary from draw to draw.

--

* Key to this observation: these are *randomly* drawn samples.

---

# Taking 18 Samples (One per Student)

* Because we can't do this activity in class, we drew 18 samples of 20 fusilli (with replacement).

--

* This is what each looked like:

---

# Taking 18 Samples (One per Student)

* Because we can't do this activity in class, we drew 18 samples of 20 fusilli (with replacement).

* For each sample, I computed the share of green fusilli.

.pull-left[
Sample # | Count | Proportion
:------:|:------:|:--------:
1 | 6 | 0.3
2 | 5 | 0.25
3 | 5 | 0.25
4 | 6 | 0.3
5 | 3 | 0.15
6 | 6 | 0.3
7 | 5 | 0.25
8 | 5 | 0.25
9 | 4 | 0.2
]

.pull-right[
Sample # | Count | Proportion
:------:|:------:|:--------:
10 | 5 | 0.25
11 | 4 | 0.2
12 | x | x
13 | x | x
14 | x | x
15 | x | x
16 | x | x
17 | x | x
18 | x | x
]

---

class: inverse

# Task 1 (10 minutes)

1. Create a vector containing the proportions of green pasta from the previous slide. Name is `pasta`. (Hint: remember that vectors are created using the `c()` function.)

1. Create a histogram of these proportions using `ggplot2`. Use these parameters in `geom_histogram()`: `boundary = 0.3, binwidth = 0.05`.

1. What do you observe?

---

# Sample Distribution: Histogram

.pull-left[
```{r, echo=FALSE, eval = TRUE, fig.height=8}
pasta_samples <- data.frame(group = 1:11, replicate = 1:11, prop_green = c(0.3,0.25,0.25,0.3,0.15,0.3,0.25,0.25,0.2,0.25,0.2))

library(tidyverse)
library(tweenr) # Available on CRAN
library(ggforce) # Install from thomasp85/ggforce
library(gganimate) # Install from dgrtwo/gganimate

# set.seed(2)
# x = c(0.3,0.25,0.25,0.3,0.15,0.3,0.25,0.25,0.2,0.25,0.2,0.2,0.35,0.25,0.3,0.35,0.4,0.2,0.25)
# 
# df <- data.frame(x = x, y = 15)
# dfs <- list(df)
# for(i in seq_len(nrow(df))) {
#     dftemp <- tail(dfs, 1)
#     dftemp[[1]]$y[i] <- sum(dftemp[[1]]$x[seq_len(i)] == dftemp[[1]]$x[i])
#     dfs <- append(dfs, dftemp)
# }
# dfs <- append(dfs, dfs[rep(length(dfs), 3)])
# dft <- tween_states(dfs, 10, 1, 'cubic-in', 200)
# dft$y <- dft$y - 0.5
# dft <- dft[dft$y != 14.5, ]
# dft$type <- 'Animate'
# 
# p <- ggplot(dft) +
#   geom_point(aes(x, y), shape = 21, colour = "black", fill = "darkgreen", size = 13, stroke = 1) +
#   labs(
#     x = "Proportion of green fusilli",
#     y = "Frequency"
#   ) +
#   ylim(0, 13.5) +
#   xlim(0, 0.5) +
#   theme_bw(base_size = 14) +
#   transition_manual(.frame)
# p
# anim_save(filename = "img/photos/hist_building.gif", animation = p)

knitr::include_graphics("../img/photos/hist_building.gif")
```
]

.pull-right[
```{r, echo=FALSE, eval = TRUE, fig.height=7}
pasta_samples %>%
  ggplot(aes(x = prop_green)) +
  geom_histogram(boundary = 0.3, binwidth = 0.05, col = "white", fill = "#d90502") +
  labs(
    x = "Proportion of green pasta",
    y = "Frequency"
  ) +
  xlim(0,0.5) +
  theme_bw(base_size = 20)
```
]

---

# Sample Distribution: Histogram with Real Proportion

.pull-left[
```{r, echo=FALSE, eval = TRUE, fig.height=8}
knitr::include_graphics("../img/photos/hist_building.gif")
```
]

.pull-right[
```{r, echo=FALSE, eval = TRUE, fig.height=7}
pasta_samples %>%
  ggplot(aes(x = prop_green)) +
  geom_histogram(boundary = 0.3, binwidth = 0.05, col = "white", fill = "#d90502") +
  labs(
    x = "Proportion of green pasta",
    y = "Frequency"
  ) +
  xlim(0,0.5) +
  theme_bw(base_size = 20)
```
]

---

# What Did We Just Do?

--

* Demonstrated the statistical concept of ***sampling***.

--

* *Objective*: know the proportion of green pasta

--

* *Methods*:

  1. **Census**: time-consuming (and in many cases very costly);
  
--
  
  1. **Sampling**: extract a *sample* of 20 pasta from the bowl to obtain an ***estimate***.  
  Out first ***estimate*** of the proportion of green pasta was 0.25.
  
--

* *Important*: each *sample* was drawn ***randomly*** $\rightarrow$ samples are different from each other! $\rightarrow$ different proportions `r emo::ji("point_right")` ***sampling variation***

---

# Handover to `Moderndive`

* `ModernDive` is a wonderful [textbook](https://moderndive.com) on and `R` package (`moderndive`) for statistical inference.

--

* In their chapter on sampling, they use red and white balls instead of fusilli.

<div align="center"><img src="../img/photos/transition.png"?></div>

* We want to know what's the proportion of **red** balls in the bowl.

* The rest is identical. We will continue with their example now!

---

# Taking Virtual (not Real) Samples

.pull-left[
* All the balls in the bowl are stored in a data.frame `bowl`.

```{r, echo = TRUE}
library(moderndive)  # load their package
head(bowl)
nrow(bowl)
```

* `ball_ID`: ball identifier

* `color`: ball color
]

--

.pull-right[
* Instead of selecting balls with our hands, we'll take *virtual* draws from the bowl.

* We'll use the *virtual shovel* to take a sample of 50 balls from our virtual bowl.
]

--

---

# Using A Virtual Shovel Once

* We will take a first sample of size 50, using the `moderndive` function `rep_sample_n`.

--

```{r}
virtual_shovel <- bowl %>% # notice that moderndive functions can be "pipped"
  rep_sample_n(size = 50) # take a sample of 50 balls

# display the sample
virtual_shovel
```


* Column `replicate` tells us the ID of the sample. Here: `1`.

---

# Proportion of *Red* Balls

.pull-left[
```{r}
virtual_shovel %>% 
  summarize(
    num_red = sum(color == "red"),
    sample_n = n()) %>% 
  mutate(prop_red = num_red / sample_n)
```
]

.pull-right[
1. Compute:
  * sum of red balls in sample,
  * number of observations in sample (i.e. 50 in this case)

1. Computes proportion of red balls

`r emo::ji("point_right")` `r sample_1$prop_red` are red! This is an ***estimate*** of the proportion of red balls in the bowl. What if we try again?

What if we try many times, like, 33 times?
]

---

# Using The Virtual Shovel 33 Times

.pull-left[

33 samples (*replicates*) of size 50.
```{r}
virtual_samples <- bowl %>%
  # get a sample of size 50, 33 times.
  rep_sample_n(size = 50, reps = 33)
virtual_samples
```
]

--

.pull-right[

Compute the proportion of red balls in each sample.

```{r}
virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% # calculate stat by sample
  summarize(
    num_red = sum(color == "red"),
    sample_n = n()) %>% 
  mutate(prop_red = num_red / sample_n)
virtual_prop_red
```
]


---

# (Virtual!) Sampling Variation

.pull-left[
* Just as when we did it, the virtual sampler *also* creates random samples.

* The `prop_red` column in the `virtual_prop_red` data.frame differs across samples.

* And again, we can visualize the ***sampling distribution***:

```{r,eval=FALSE}
ggplot(virtual_prop_red, aes(x = prop_red)) +
  geom_histogram(binwidth = 0.05, 
                 boundary = 0.4,
                 color = "white",
                 fill = "#d90502") +
  labs(x = "Proportion of 50 balls that were red",
       y = "Frequency",
       title = "Distribution of 33 samples of size 50") +
  theme_bw(base_size = 20)
```
]

--

.pull-right[
```{r,echo = FALSE,fig.height=6}
ggplot(virtual_prop_red, aes(x = prop_red)) +
  geom_histogram(binwidth = 0.05, boundary = 0.4, color = "white", fill = "#d90502") +
  labs(x = "Proportion of red balls in sample", 
       y = "Frequency",
       title = "Distribution for 33 samples of size 50") +
  theme_bw(base_size = 20)
```
]

---

class: inverse

# Task 2 (10 minutes)

Instead of taking only 33 samples, let's take ***1000***!

1. Why do we not take 1000 samples "by hand"?

1. Obtain 1000 samples of size 50 using the `rep_sample_n()` function from the `moderndive` package.

1. Calculate the proportion of red balls in each sample.

1. Plot a histogram of the obtained proportion of red balls in each sample.

1. What do you observe? Which proportions occur most frequently? How does the shape of the histogram compare to when we took only 33 samples?

1. How likely is it that we sample 50 balls of which less than 20% are red?

---

# Sampling Distribution of 1000 Samples

```{r, echo = FALSE, eval = TRUE, fig.height = 4.75, fig.width = 8}
virtual_samples <- bowl %>% 
  rep_sample_n(size = 50, reps = 1000)

virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% 
  summarize(
    num_red = sum(color == "red"),
    sample_n = n()) %>% 
  mutate(prop_red = num_red / sample_n)

virtual_prop_red %>% ggplot(
  aes(x = prop_red)) +
  geom_histogram(
    binwidth = 0.05,
    boundary = 0.4,
    color = "white",
    fill = "#d90502") +
  labs(x = "Proportion of red balls in sample",
       y = "Frequency",
       title = "Distribution of 1000 samples of size 50") +
  theme_bw(base_size = 14)
```

Looks remarkably close to a ***normal distribution*** $\rightarrow$ the more samples we take, the more their ***sampling distribution*** will resemble a ***normal distribution***.

---

# Role of Sample Size

Imagine you could change the size of your samples and had the option of the following sizes: 25, 50 and 100.

If your goal is still to estimate the proportion of the bowl’s balls that are red, which shovel would you choose?

---

# Role of Sample Size

* Let's repeat what we did previously but for different sample sizes.

* Let's take 1000 samples each for $n=25,n=50,n=100$.

--

* We will use `rep_sample_n()` again.

--

.pull-left[

Generate all samples of different sizes:

```{r}
# Sample size: 25
virtual_samples_25 <- bowl %>% 
  rep_sample_n(size = 25, reps = 1000)

# Sample size: 50
virtual_samples_50 <- bowl %>% 
  rep_sample_n(size = 50, reps = 1000)

# Sample size: 100
virtual_samples_100 <- bowl %>% 
  rep_sample_n(size = 100, reps = 1000)
```
]

--

.pull-right[

Compute proportion of red balls:

```{r}
# Sample size: 25
# The same code is used for the other sample sizes
virtual_prop_red_25 <- virtual_samples_25 %>% 
  group_by(replicate) %>% 
  summarize(
    num_red = sum(color == "red"),
    sample_n = n()) %>% 
  mutate(prop_red = num_red / sample_n)
```
]

---

# Role of Sample Size

```{r, echo = FALSE, fig.height=5, fig.width=10}

# Sample size: 50
virtual_prop_red_50 <- virtual_samples_50 %>% 
  group_by(replicate) %>% 
  summarize(
    num_red = sum(color == "red"),
    sample_n = n()) %>% 
  mutate(prop_red = num_red / sample_n)

# Sample size: 100
virtual_prop_red_100 <- virtual_samples_100 %>% 
  group_by(replicate) %>% 
  summarize(
    num_red = sum(color == "red"),
    sample_n = n()) %>% 
  mutate(prop_red = num_red / sample_n)

df = rbind(virtual_prop_red_25,virtual_prop_red_50,virtual_prop_red_100)

df %>% 
  ggplot(aes(x = prop_red)) +
    geom_histogram(binwidth = 0.05, boundary = 0.4, color = "white", fill = "#d90502") +
  scale_x_continuous(breaks = round(seq(0.2, 0.6, by = 0.1),1)) +
  labs(
    x = "Proportion of red balls in sample",
    y = "Frequency",
    title = "Comparing distributions of proportions of red balls for different sample sizes"
  ) +
    facet_wrap(~sample_n, labeller = as_labeller(
        c(`25` = "Samples of size 25",
          `50` = "Samples of size 50",
          `100` = "Samples of size 100"))) +
    theme_bw(base_size = 14)
```

---


# Sample Size and Sampling Distributions

* The larger the sample size, the *narrower* the resulting sampling distribution.

--

* In other words, there are fewer differences due to sampling variation.

--

* Holding constant the number of replicates (i.e. 1000 in our case), ***bigger samples*** will yield *normal distributions* with ***smaller standard deviations***.

--

```{r, echo = FALSE}
# n = 25
sd25 = virtual_prop_red_25 %>% 
  summarize(sd = sd(prop_red))

# n = 50
sd50 = virtual_prop_red_50 %>% 
  summarize(sd = sd(prop_red))

# n = 100
sd100 = virtual_prop_red_100 %>% 
  summarize(sd = sd(prop_red))
```

Sample Size | Standard Deviation
:---------:|:--------------:
25          | `r round(sd25$sd,2)`
50          | `r round(sd50$sd,2)`
100         |`r round(sd100$sd,2)`

--

* Remember that the ***standard deviation*** measures the *spread* of a variable around its mean.

--

* So as the sample size increases, our ***estimates*** of the true proportion of the bowl's red balls get more *precise*.

---

# Sampling Framework

* We used sampling for the purpose of ***estimation***.

--

* We extracted samples in order to ***estimate*** the proportion of the bowl's balls that are red.

--

* 2 key concepts relating to sampling for estimation:

  1. The effect of *sampling variation* on our estimates.
  
  1. The effect of sample size on *sampling variation*.


---


# Sampling Glossary `r emo::ji("book")`

.pull-left[
***Population:*** collection of individuals or observations we are interested in.  
$N = 2400$ balls.

***Population parameter:*** numerical summary quantity about the population that is unknown but that we want to know.  
*Examples:* population mean $(\mu)$, proportion of red balls $(p)$.

***Census:*** exhaustive enumeration or counting of all $N$ individuals or observations in the population in order to compute the population parameter’s value *exactly*.

***Sampling:*** dollecting samples of size $n$ from the population of size $N$.
]

--

.pull-right[
* ***Point estimate*** or ***Sample statistic:*** summary statistic computed from a sample that estimates an unknown population parameter.  
*Example:* *sample proportion* of red balls $(\hat{p})$. The "hat" on top of the $p$ indicates that it is an *estimate* of the population proportion $p$.

* ***Representative sampling:*** does the sample *look like* the population?

* ***Biased sampling:*** did all balls have an equal chance of being included in a sample?

* ***Random sampling:*** randomly sampling in an unbiased fashion.

]

---

# Statistical Definitions

* We have been estimating $\hat{p}$ all along.

--

* We plotted the *sampling distribution* to display the sampling variation of the sample proportion $\hat{p}$.

--

* We computed the *standard deviation* of the *sampling distribution* of $\hat{p}$. This standard deviation has a special name: ***standard error*** of the *point estimate* $\hat{p}$.

--

* Let's reproduce the summary table and labelling properly:

Sample Size $(n)$ | Standard Error of $\hat{p}$
:---------:|:--------------:
25          | `r round(sd25$sd,2)`
50          | `r round(sd50$sd,2)`
100         |`r round(sd100$sd,2)`

* Key takeaway: as the *sample size* $n$ goes up, the “typical” error of your *point estimate* will go down, as quantified by the *standard error*.

---

# Putting It All Together

* ***Point estimates*** from ***random samples*** provide a *good guess* of the true unknown ***population parameter***.

--

* How good? Sometimes $\hat{p}$ will be far from $p$, sometimes close. There's ***sampling variation***.

--

* ***On average***, our estimates will be correct. This is because of random sampling. We say that: 
> ### $\hat{p}$ is an ***unbiased estimator*** of $p$, i.e. $\mathop{\mathbb{E}}[\hat{p}] = p$

--

* What is the true population proportion $p$ of red balls in the population of $N=2400$ balls?

--

```{r}
sum(bowl$color == "red")/nrow(bowl)
```

--

* Let's insert the ***true population proportion*** $p=0.375$ into our previous plots!

---

# Visualizing Unbiasedness and Sampling Variation

```{r,echo = FALSE,fig.width=10,fig.height=5}
p <- bowl %>% 
  summarize(p = mean(color == "red")) %>% 
  pull(p)
df %>% 
  ggplot(aes(x = prop_red)) +
    geom_histogram(binwidth = 0.05, boundary = 0.4, color = "white", fill = "#d90502") +
    scale_x_continuous(breaks = round(seq(0.2, 0.6, by = 0.1),1)) +
    labs(
      x = expression(paste("Sample proportion ", hat(p))),
      y = "Frequency",
      title = expression(paste("Sampling distributions of ", hat(p), " based on n = 25, 50, 100."))) +
  facet_wrap(~sample_n, labeller = as_labeller(
      c(`25` = "n = 25",
          `50` = "n = 50",
          `100` = "n = 100"))) +
  theme_bw(base_size = 14) +
  geom_vline(xintercept = p, col = "black", size = 0.75) +
  annotate("text", x = 0.56, y = 410, label = "True population proportion", size = 3)
```

---
# Some Sampling Scenarios


Scenario | Population parameter | Notation | Point estimate | Symbol(s)
:--: | :--: | :--: |:--: | :--:
1 | Population proportion | $p$ | Sample proportion | $\widehat{p}$
2 | Population mean | $\mu$ | Sample mean | $\overline{x}$ or $\widehat{\mu}$
3 | Difference in population proportions | $p_1 - p_2$ | Difference in sample proportions | $\widehat{p}_1 - \widehat{p}_2$
4 | Difference in population means | $\mu_1 - \mu_2$ | Difference in sample means | $\overline{x}_1 - \overline{x}_2$
5 | Population regression slope | $\beta_1$ | Fitted regression slope | $b_1$ or $\widehat{\beta}_1$
6 | Population regression intercept | $\beta_0$ | Fitted regression intercept | $b_0$ or $\widehat{\beta}_0$

---

# The Central Limit Theorem (CLT)

* The fact that our sample statistics ***converge*** to a *central limit* is well known in statistics.

--

* It's due to a famous result know as the ***central limit theorem***.

--

* It states that regardless of how the underlying population distribution looks like, when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow.

* In other words, their sampling distribution increasingly follows a ***normal distribution*** and the *variation of these sampling distributions gets smaller*, as quantified by their ***standard errors***.

---

# Central Limit Theorem - NYTimes video

.center[
<iframe width="684" height="464" src="https://www.youtube.com/embed/jvoxEYmQHNM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
]

---

class: title-slide-final, middle

# THANKS

To the amazing [moderndive](https://moderndive.com/) team!

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# SEE YOU NEXT WEEK!




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

